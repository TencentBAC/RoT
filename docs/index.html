<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: white;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        
        header {
            text-align: center;
            padding: 40px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-bottom: 40px;
        }
        
        h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 700;
        }
        
        .authors {
            font-size: 1.1em;
            margin: 15px 0;
            font-style: italic;
        }
        
        .affiliations {
            font-size: 0.95em;
            margin: 10px 0;
            opacity: 0.9;
        }
        
        .badges {
            margin: 30px 0;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .badge {
            display: inline-block;
            padding: 8px 16px;
            background-color: rgba(255, 255, 255, 0.2);
            border-radius: 5px;
            text-decoration: none;
            color: white;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        
        .badge:hover {
            background-color: rgba(255, 255, 255, 0.3);
        }
        
        section {
            margin: 40px 0;
            padding: 30px;
            background-color: #fafafa;
            border-radius: 8px;
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
        }
        
        .abstract {
            background-color: #e8f4f8;
            padding: 25px;
            border-left: 5px solid #667eea;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .figure {
            text-align: center;
            margin: 30px 0;
        }
        
        .figure img,
        .figure embed,
        .figure object {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .figure embed,
        .figure object {
            min-height: 400px;
            width: 100%;
        }
        
        .figure-caption {
            margin-top: 15px;
            font-style: italic;
            color: #666;
            font-size: 0.95em;
        }
        
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .feature-card {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.3s;
        }
        
        .feature-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
        
        .feature-card h4 {
            color: #667eea;
            margin-bottom: 10px;
        }
        
        ul {
            margin: 15px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .citation {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            overflow-x: auto;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            margin-top: 40px;
            border-top: 2px solid #eee;
            color: #666;
        }
        
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }
            
            .container {
                padding: 10px;
            }
            
            section {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</h1>
            <div class="authors">
                Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei
            </div>
            <div class="affiliations">
                Basic Algorithm Center, PCG, Tencent
            </div>
            <div class="badges">
                <a href="https://arxiv.org/abs/2601.14750" class="badge">ðŸ“„ Paper</a>
                <a href="https://github.com/TencentBAC/RoT" class="badge">ðŸ’» Code</a>
                <a href="https://huggingface.co/TencentBAC/RoT" class="badge">ðŸ¤— Models</a>
            </div>
        </header>

        <section id="abstract">
            <h2>Abstract</h2>
            <div class="abstract">
                <p>
                    Render-of-Thought (RoT) introduces a paradigm shift in latent reasoning by visualizing the reasoning chain. 
                    Instead of compressing Chain-of-Thought (CoT) into opaque vectors, RoT renders textual reasoning steps into images 
                    and uses pre-trained vision encoders as semantic anchors to guide the reasoning process. This approach achieves:
                </p>
                <ul>
                    <li><strong>3-4Ã— token compression</strong> compared to explicit CoT</li>
                    <li><strong>Significant inference acceleration</strong> while maintaining competitive accuracy</li>
                    <li><strong>Interpretable reasoning</strong> through visual representations</li>
                    <li><strong>Plug-and-play implementation</strong> without additional pre-training overhead</li>
                </ul>
            </div>
        </section>

        <section id="overview">
            <h2>Overview</h2>
            <div class="figure">
                <embed src="fig/overview.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 1:</strong> Overview of the Render-of-Thought framework. 
                    (a) Rendering Method transforms textual reasoning steps into compact single-line images. 
                    (b) Latent Reasoning Method aligns LLM-generated hidden states with visual features via a projection head, 
                    enabling the model to perform continuous reasoning within the visual latent space.
                </div>
            </div>
        </section>

        <section id="introduction">
            <h2>Introduction</h2>
            <div class="figure">
                <embed src="fig/introduction.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 2:</strong> Introduction and motivation for Render-of-Thought.
                </div>
            </div>
        </section>

        <section id="method">
            <h2>Method</h2>
            <h3>Key Features</h3>
            <div class="features">
                <div class="feature-card">
                    <h4>ðŸŽ¨ Text-to-Image Rendering</h4>
                    <p>Converts textual CoT steps into compact single-line images with dynamic width and fixed height.</p>
                </div>
                <div class="feature-card">
                    <h4>ðŸ”— Visual-Semantic Alignment</h4>
                    <p>Aligns LLM hidden states with visual embeddings via a projection head, using pre-trained vision encoders as semantic anchors.</p>
                </div>
                <div class="feature-card">
                    <h4>ðŸš€ Two-Stage Training</h4>
                    <p>Stage 1: Train projection head to align latent representations. Stage 2: Fine-tune language model head.</p>
                </div>
            </div>

            <h3>Training Pipeline</h3>
            <div class="figure">
                <embed src="fig/training_stage.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 3:</strong> Two-stage training pipeline of Render-of-Thought.
                </div>
            </div>
        </section>

        <section id="results">
            <h2>Experimental Results</h2>
            
            <h3>Performance on Mathematical Reasoning</h3>
            <div class="figure">
                <img src="fig/math.png" alt="Mathematical Reasoning Results" onerror="this.style.display='none'">
                <div class="figure-caption">
                    <strong>Figure 4:</strong> Performance comparison on mathematical reasoning benchmarks.
                </div>
            </div>

            <h3>GSM8K-Aug Results</h3>
            <div class="figure">
                <img src="fig/gsm8k-aug.png" alt="GSM8K-Aug Results" onerror="this.style.display='none'">
                <div class="figure-caption">
                    <strong>Figure 5:</strong> Results on GSM8K-Aug dataset.
                </div>
            </div>

            <h3>Inference Time</h3>
            <div class="figure">
                <embed src="fig/infer_time.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 6:</strong> Inference time comparison with baseline methods.
                </div>
            </div>

            <h3>Loss Curves</h3>
            <div class="figure">
                <embed src="fig/loss_curve.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 7:</strong> Training loss curves for Stage 1 and Stage 2.
                </div>
            </div>

            <h3>Decoding Strategies</h3>
            <div class="figure">
                <embed src="fig/decoding_strategies.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 8:</strong> Comparison of different decoding strategies.
                </div>
            </div>
        </section>

        <section id="visualization">
            <h2>Visualization Examples</h2>
            <div class="figure">
                <embed src="fig/vis_case.pdf" type="application/pdf" width="100%" height="600px">
                <div class="figure-caption">
                    <strong>Figure 9:</strong> Visualization examples of rendered reasoning chains.
                </div>
            </div>
        </section>

        <section id="failure">
            <h2>Failure Case Analysis</h2>
            <div class="figure">
                <img src="fig/failure_case.png" alt="Failure Cases" onerror="this.style.display='none'">
                <div class="figure-caption">
                    <strong>Figure 10:</strong> Analysis of failure cases and error patterns.
                </div>
            </div>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <div class="citation">
@article{wang2026rot,
  title={Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning},
  author={Yifan Wang and Shiyu Li and Peiming Li and Xiaochen Yang and Yang Tang and Zheng Wei},
  journal={arXiv preprint arXiv:2601.14750},
  year={2026}
}
            </div>
        </section>

        <footer>
            <p>This page was built using the Academic Project Page Template. The source code is available on <a href="https://github.com/TencentBAC/RoT">GitHub</a>.</p>
            <p style="margin-top: 10px;">Â© 2026 Render-of-Thought. All rights reserved.</p>
        </footer>
    </div>
</body>
</html>
