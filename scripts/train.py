"""
è®­ç»ƒè„šæœ¬

è®­ç»ƒ CoT å‹ç¼©å™¨æ¨¡å‹
"""

import os
import sys
import json
import yaml
import argparse
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from transformers import PreTrainedModel
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import shutil

# DeepSpeed æ”¯æŒ
try:
    import deepspeed

    DEEPSPEED_AVAILABLE = True

    # æŠ‘åˆ¶ DeepSpeed çš„ INFO çº§åˆ«æ—¥å¿—ï¼ˆé¿å… MPI æ£€æµ‹ç­‰è­¦å‘Šï¼‰
    import logging

    # è®¾ç½® deepspeed ç›¸å…³ logger çš„çº§åˆ«
    deepspeed_logger = logging.getLogger("deepspeed")
    deepspeed_logger.setLevel(logging.WARNING)

    # ä¹ŸæŠ‘åˆ¶ deepspeed.comm çš„æ—¥å¿—ï¼ˆè¿™æ˜¯ MPI æ£€æµ‹æ—¥å¿—çš„æ¥æºï¼‰
    comm_logger = logging.getLogger("deepspeed.comm")
    comm_logger.setLevel(logging.ERROR)  # è®¾ç½®ä¸º ERROR ä»¥å®Œå…¨éšè— INFO æ¶ˆæ¯

    # æŠ‘åˆ¶ root logger ä¸­å¯èƒ½çš„è·¯ç”±
    logging.getLogger("deepspeed.utils").setLevel(logging.WARNING)
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("Warning: DeepSpeed not available. Install with: pip install deepspeed")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.cot_compressor import CoTCompressor
from models.cot_compressor_v2 import CoTCompressorV2


class CoTDataset(Dataset):
    """CoT æ•°æ®é›†"""

    def __init__(self, data_file: str, tokenizer=None, silent: bool = False):
        """
        Args:
            data_file: é¢„å¤„ç†åçš„æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: tokenizerå¯¹è±¡ï¼Œç”¨äºè·å–eos_token_idå’Œåº”ç”¨chat template
            silent: æ˜¯å¦é™é»˜æ¨¡å¼ï¼ˆå¤šå¡è®­ç»ƒæ—¶é¿å…é‡å¤è¾“å‡ºï¼‰
        """
        self.data = []
        self.tokenizer = tokenizer
        
        with open(data_file, "r", encoding="utf-8") as f:
            for line in f:
                self.data.append(json.loads(line.strip()))

        if not silent:
            print(f"âœ“ Loaded {len(self.data)} samples from {data_file}")
            if tokenizer is not None:
                eos_token = tokenizer.eos_token if hasattr(tokenizer, 'eos_token') else None
                print(f"  Using Qwen3-VL chat template with EOS token: {eos_token}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        
        question = item["question"]
        cot = item["cot"]
        answer = item["answer"]
        
        # åº”ç”¨Qwen3-VLçš„chat templateæ ¼å¼
        # æ ¼å¼ï¼š<|im_start|>system\nç³»ç»Ÿæç¤º<|im_end|>\n<|im_start|>user\né—®é¢˜<|im_end|>\n<|im_start|>assistant\nå›ç­”<|im_end|>
        # "You are a helpful assistant. The final output format is as follows: Answer: <answer>. <|im_end|>\n"
        
        # æ„å»ºæ ¼å¼åŒ–çš„questionï¼ˆåŒ…å«systemå’Œuseréƒ¨åˆ†ï¼‰
        '''
        formatted_question = (
            "<|im_start|>system\n"
            "You are a helpful assistant. The final output format is as follows: Answer: <answer>. <|im_end|>\n"
            f"<|im_start|>user\n{question}<|im_end|>\n"
            "<|im_start|>assistant\n"
        )
        '''
        formatted_question = question
        # æ„å»ºæ ¼å¼åŒ–çš„CoTï¼ˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹ï¼‰
        formatted_cot = cot
        # æ„å»ºæ ¼å¼åŒ–çš„answerï¼Œåœ¨æœ«å°¾æ·»åŠ <|im_end|>
        # <|im_end|>åœ¨Qwen3-VLä¸­ä¼šè¢«tokenizerè½¬æ¢ä¸ºeos_token_id
        # formatted_answer = "### " + answer + "<|im_end|>"
        formatted_answer = "### " + answer + " "
        return {
            "question": formatted_question,
            "cot": formatted_cot,
            "answer": formatted_answer,
        }


def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, List[str]]:
    """
    æ‰¹é‡æ•°æ®æ•´ç†
    ç”±äºæ¯ä¸ªæ ·æœ¬çš„ CoT é•¿åº¦ä¸åŒï¼Œæˆ‘ä»¬ä¸åš paddingï¼Œç›´æ¥è¿”å›åˆ—è¡¨
    """
    return {
        "questions": [item["question"] for item in batch],
        "cots": [item["cot"] for item in batch],
        "answers": [item["answer"] for item in batch],
    }


class Trainer:
    """è®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any], deepspeed_config_path: Optional[str] = None):
        self.config = config
        self.deepspeed_config_path = deepspeed_config_path

        # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
        self.local_rank = int(os.environ.get("LOCAL_RANK", -1))
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))
        self.use_deepspeed = (
            DEEPSPEED_AVAILABLE
            and self.deepspeed_config_path is not None
            and os.path.exists(self.deepspeed_config_path)
            and self.local_rank >= 0
        )

        if self.use_deepspeed:
            # DeepSpeed ä¼šè‡ªåŠ¨åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆé€šè¿‡ deepspeed.initializeï¼‰
            # DeepSpeed launcher é€šå¸¸ä¼šè‡ªåŠ¨è®¾ç½® MASTER_ADDR å’Œ MASTER_PORT
            # å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œæˆ‘ä»¬æ‰è®¾ç½®é»˜è®¤å€¼
            if "MASTER_ADDR" not in os.environ:
                os.environ["MASTER_ADDR"] = "localhost"
            
            # è‡ªåŠ¨é€‰æ‹©å¯ç”¨ç«¯å£ï¼ˆä»…å½“ MASTER_PORT æœªè®¾ç½®æ—¶ï¼‰
            # æ³¨æ„ï¼šDeepSpeed launcher åº”è¯¥ä¼šè‡ªåŠ¨è®¾ç½® MASTER_PORT
            # å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäº PID å’Œæ—¶é—´çš„ç«¯å£é€‰æ‹©
            if "MASTER_PORT" not in os.environ:
                import socket
                import os as os_module
                
                # æ‰€æœ‰è¿›ç¨‹éœ€è¦ä½¿ç”¨ç›¸åŒçš„ç«¯å£ï¼Œæ‰€ä»¥ä½¿ç”¨ä¸€ä¸ªç¡®å®šæ€§çš„æ–¹æ³•
                # åŸºäºè¿›ç¨‹ç»„çš„ PIDï¼ˆä¸»è¿›ç¨‹çš„ PIDï¼‰å’Œæ—¶é—´æˆ³æ¥é€‰æ‹©ç«¯å£
                base_port = 29500
                # ä½¿ç”¨ä¸»è¿›ç¨‹çš„ PIDï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–è€…å½“å‰è¿›ç¨‹ PID
                # å¯¹äº DeepSpeedï¼Œæ‰€æœ‰å­è¿›ç¨‹çš„çˆ¶è¿›ç¨‹ PID ç›¸åŒ
                try:
                    # å°è¯•è·å–çˆ¶è¿›ç¨‹ PIDï¼ˆåœ¨ DeepSpeed ä¸­ï¼Œæ‰€æœ‰è¿›ç¨‹å…±äº«åŒä¸€ä¸ªçˆ¶è¿›ç¨‹ï¼‰
                    parent_pid = os_module.getppid()
                    pid = parent_pid
                except:
                    pid = os_module.getpid()
                
                # ä½¿ç”¨ PID å’Œæ—¶é—´æˆ³ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„èµ·å§‹ç«¯å£
                import time
                time_hash = int(time.time()) % 1000
                port_offset = (pid + time_hash) % 400  # é™åˆ¶åœ¨ 29500-29899 èŒƒå›´å†…
                start_port = base_port + port_offset
                
                # å°è¯•ä»èµ·å§‹ç«¯å£å¼€å§‹æŸ¥æ‰¾å¯ç”¨ç«¯å£ï¼ˆæœ€å¤šå°è¯• 50 ä¸ªï¼‰
                selected_port = None
                for i in range(50):
                    port = start_port + i
                    if port > 29999:  # é™åˆ¶æœ€å¤§ç«¯å£
                        port = base_port + (port - 29999 - 1)
                    
                    try:
                        # å°è¯•ç»‘å®šç«¯å£æ¥æ£€æŸ¥æ˜¯å¦å¯ç”¨ï¼ˆåªæ£€æŸ¥ï¼Œä¸å ç”¨ï¼‰
                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                            s.bind(("localhost", port))
                            selected_port = port
                            if self.local_rank == 0:
                                print(f"âœ“ Auto-selected MASTER_PORT={port} (from PID {pid})")
                            break
                    except (OSError, socket.error):
                        # ç«¯å£è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                        continue
                
                # å¦‚æœæ‰¾ä¸åˆ°å¯ç”¨ç«¯å£ï¼Œä½¿ç”¨åŸºäº PID çš„ç¡®å®šæ€§ç«¯å£ï¼ˆå³ä½¿å¯èƒ½è¢«å ç”¨ï¼‰
                if selected_port is None:
                    selected_port = base_port + (pid % 400)
                    if self.local_rank == 0:
                        print(f"âš ï¸  Warning: Using deterministic port {selected_port} based on PID")
                
                # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦ï¼‰
                os.environ["MASTER_PORT"] = str(selected_port)

            # è®¾ç½®æ ‡å¿—å‘Šè¯‰ DeepSpeed æˆ‘ä»¬ä½¿ç”¨äº† DeepSpeed launcher
            # DeepSpeed é€šè¿‡æ£€æŸ¥è¿™äº›ç¯å¢ƒå˜é‡æ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº† launcher
            os.environ["DEEPSPEED_LAUNCHER"] = "1"

            # æ£€æŸ¥å¹¶è®¾ç½® NCCL è¶…æ—¶æ—¶é—´ï¼ˆå¦‚æœæœªè®¾ç½®ï¼Œè®¾ç½®ä¸€ä¸ªè¾ƒé•¿çš„é»˜è®¤å€¼ï¼‰
            # DeepSpeed ZeRO Stage 2 çš„æ£€æŸ¥ç‚¹ä¿å­˜éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œéœ€è¦è¶³å¤Ÿé•¿çš„è¶…æ—¶
            if "NCCL_TIMEOUT" not in os.environ:
                # è®¾ç½®é»˜è®¤è¶…æ—¶ä¸º 30 åˆ†é’Ÿï¼ˆ1800 ç§’ï¼‰
                os.environ["NCCL_TIMEOUT"] = "1800"
                if self.local_rank == 0:
                    print("  Note: NCCL_TIMEOUT not set, using default 1800 seconds")
            else:
                nccl_timeout = int(os.environ.get("NCCL_TIMEOUT", "1800"))
                if nccl_timeout < 1800 and self.local_rank == 0:
                    print(f"  Warning: NCCL_TIMEOUT={nccl_timeout} may be too short for DeepSpeed checkpoint saving")
                    print(f"  Recommended: export NCCL_TIMEOUT=1800 or higher")

            # ä½†æˆ‘ä»¬éœ€è¦å…ˆè®¾ç½®è®¾å¤‡ï¼ˆåœ¨ deepspeed.initialize ä¹‹å‰ï¼‰
            torch.cuda.set_device(self.local_rank)
            self.device = torch.device(f"cuda:{self.local_rank}")
            self.is_main_process = self.local_rank == 0

            # é‡è¦ï¼šä¸è¦åœ¨ DeepSpeed æ¨¡å¼ä¸‹æ‰‹åŠ¨åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
            # DeepSpeed ä¼šåœ¨ deepspeed.initialize() æ—¶è‡ªåŠ¨åˆå§‹åŒ–
        elif self.local_rank >= 0:
            # ä½¿ç”¨æ ‡å‡†çš„ PyTorch DDP
            torch.distributed.init_process_group(backend="nccl")
            torch.cuda.set_device(self.local_rank)
            self.device = torch.device(f"cuda:{self.local_rank}")
            self.is_main_process = self.local_rank == 0
        else:
            # å•å¡è®­ç»ƒ
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.is_main_process = True

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config["logging"]["checkpoint_dir"])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.log_dir = Path(config["logging"]["log_dir"])
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ– TensorBoardï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            try:
                self.tensorboard_dir = self.log_dir / "tensorboard"
                self.tensorboard_dir.mkdir(parents=True, exist_ok=True)
                self.writer = SummaryWriter(log_dir=str(self.tensorboard_dir))
                print(f"âœ“ TensorBoard logging to: {self.tensorboard_dir}", flush=True)
                print(f"  Log directory: {self.log_dir.absolute()}", flush=True)
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to initialize TensorBoard: {e}", flush=True)
                print(f"  Continuing without TensorBoard logging...", flush=True)
                self.writer = None
        else:
            self.writer = None

        # åˆå§‹åŒ–æ¨¡å‹
        if self.is_main_process:
            print("\n" + "=" * 80)
            print("Initializing Model")
            print("=" * 80)

        # å¯¹äº DeepSpeedï¼Œå…ˆåˆå§‹åŒ–æ¨¡å‹åˆ° CPUï¼Œç„¶åè®© DeepSpeed è‡ªåŠ¨åˆ†é…åˆ° GPU
        # å¯¹äºé DeepSpeedï¼Œç›´æ¥æ”¾åˆ°æŒ‡å®šè®¾å¤‡
        model_device = "cpu" if self.use_deepspeed else self.device

        self.model = CoTCompressorV2(
            ocr_model_path=config["ocr_model"]["model_path"],
            llm_model_path=config["llm_model"]["model_path"],
            image_size=config["rendering"]["image_size"],
            font_size=config["rendering"]["font_size"],
            device=model_device,  # DeepSpeed æ¨¡å¼ä¸‹å…ˆç”¨ CPUï¼ŒDeepSpeed ä¼šè‡ªåŠ¨åˆ†é…åˆ° GPU
            freeze_vision=config["training"].get("freeze_vision", True),
            use_projection_head=config["training"].get("use_projection_head", True),
            projection_hidden_dim=config["training"].get("projection_hidden_dim", 2048),
            enable_lora=config["training"].get("enable_lora", True),
            lora_r=config["training"].get("lora_r", 16),
            lora_alpha=config["training"].get("lora_alpha", 32),
            lora_dropout=config["training"].get("lora_dropout", 0.05),
            lora_target_modules=config["training"].get("lora_target_modules", None),
            # æŸå¤±æƒé‡
            use_uncertainty_weighting=config["training"].get("use_uncertainty_weighting", True),
            vision_loss_weight=config["training"].get("loss_weights", {}).get("vision_loss_weight", 1.0),
            lm_loss_weight=config["training"].get("loss_weights", {}).get("lm_loss_weight", 1.0),
            use_custom_llm=config["llm_model"].get("use_custom_llm", False),
            loss_type=config["training"].get("loss_type", "stable_similarity"),  # ä¼ é€’æŸå¤±ç±»å‹
            # ç¬¬äºŒé˜¶æ®µè®­ç»ƒå‚æ•°
            stage2_mode=config["training"].get("stage2_mode", False),
            train_lm_head_only=config["training"].get("train_lm_head_only", False),
            freeze_projection_head=config["training"].get("freeze_projection_head", False),
            include_img_end_loss=config["training"].get("include_img_end_loss", False),
            include_vision_loss=config["training"].get("include_vision_loss", True),
            full_finetuning=config["training"].get("full_finetuning", False),  # æ–°å¢ï¼šå…¨å‚æ•°å¾®è°ƒ
        )

        # é DeepSpeed æ¨¡å¼ä¸‹æ‰æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if not self.use_deepspeed:
            self.model = self.model.to(self.device)

        # Lossæƒé‡
        self.vision_loss_weight = config["training"]["loss_weights"].get("vision_loss_weight", 1.0)
        self.lm_loss_weight = config["training"]["loss_weights"].get("lm_loss_weight", 1.0)

        if self.is_main_process:
            print(f"  Model config:")
            print(f"    - Use projection head: {config['training'].get('use_projection_head', True)}")
            print(f"    - Projection hidden dim: {config['training'].get('projection_hidden_dim', 2048)}")
            print(f"    - Freeze vision: {config['training'].get('freeze_vision', True)}")
            print(f"    - Enable LoRA: {config['training'].get('enable_lora', True)}")
            if config["training"].get("enable_lora", True):
                print(f"    - LoRA r: {config['training'].get('lora_r', 16)}")
                print(f"    - LoRA alpha: {config['training'].get('lora_alpha', 32)}")
                print(f"    - LoRA dropout: {config['training'].get('lora_dropout', 0.05)}")
            print(f"    - Vision loss weight: {self.vision_loss_weight}")
            print(f"    - LM loss weight: {self.lm_loss_weight}")

        # è·å–tokenizerï¼ˆéœ€è¦åœ¨åŠ è½½æ•°æ®ä¹‹å‰ï¼‰
        self.tokenizer = self.model.tokenizer
        
        # åŠ è½½æ•°æ®
        if self.is_main_process:
            print("\n" + "=" * 80, flush=True)
            print("Loading Data", flush=True)
            print("=" * 80, flush=True)
        train_file = Path(config["data"]["processed_dir"]) / f"{config['data']['dataset_name']}_train_processed.jsonl"
        # éä¸»è¿›ç¨‹ä½¿ç”¨é™é»˜æ¨¡å¼ï¼Œä¼ å…¥tokenizerä»¥åº”ç”¨chat template
        self.train_dataset = CoTDataset(train_file, tokenizer=self.tokenizer, silent=not self.is_main_process)

        # è®¡ç®—è®­ç»ƒæ­¥æ•°ï¼ˆéœ€è¦å…ˆåˆ›å»ºä¸´æ—¶ DataLoader æˆ–ä½¿ç”¨æ•°æ®é›†é•¿åº¦ï¼‰
        # å¯¹äº DeepSpeedï¼Œæˆ‘ä»¬éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶ DataLoader æ¥è®¡ç®—æ­¥æ•°
        if self.use_deepspeed:
            # DeepSpeed æ¨¡å¼ä¸‹ç¨ååˆ›å»º DataLoaderï¼ˆéœ€è¦ DistributedSamplerï¼‰
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ DataLoader æ¥è®¡ç®—è®­ç»ƒæ­¥æ•°
            temp_loader = DataLoader(
                self.train_dataset,
                batch_size=config["training"]["batch_size"],
                shuffle=False,  # ä¸´æ—¶ï¼Œä¸éœ€è¦é‡‡æ ·
                collate_fn=collate_fn,
                num_workers=0,  # ä¸´æ—¶ï¼Œä¸éœ€è¦å¤šè¿›ç¨‹
            )
            num_training_steps = len(temp_loader) * config["training"]["num_epochs"]
            self.train_loader = None  # ç¨ååœ¨ DeepSpeed åˆå§‹åŒ–ååˆ›å»º
        else:
            # æ ‡å‡†è®­ç»ƒæ¨¡å¼
            if self.local_rank >= 0:
                # å¤šå¡ DDP æ¨¡å¼ï¼Œä½¿ç”¨ DistributedSampler
                from torch.utils.data.distributed import DistributedSampler

                train_sampler = DistributedSampler(
                    self.train_dataset,
                    num_replicas=self.world_size,
                    rank=self.local_rank,
                    shuffle=True,
                )
                self.train_sampler = train_sampler  # ä¿å­˜å¼•ç”¨ï¼Œç”¨äºåœ¨æ¯ä¸ª epoch è°ƒç”¨ set_epoch
                self.train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=config["training"]["batch_size"],
                    sampler=train_sampler,
                    collate_fn=collate_fn,
                    num_workers=config["misc"]["num_workers"],
                    pin_memory=config["training"].get("dataloader_pin_memory", True),
                )
            else:
                # å•å¡æ¨¡å¼
                self.train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=config["training"]["batch_size"],
                    shuffle=True,
                    collate_fn=collate_fn,
                    num_workers=config["misc"]["num_workers"],
                    pin_memory=config["training"].get("dataloader_pin_memory", True),
                )
            num_training_steps = len(self.train_loader) * config["training"]["num_epochs"]

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆDeepSpeed ä¼šè‡ªåŠ¨ç®¡ç†ï¼‰
        if self.use_deepspeed:
            # åˆå§‹åŒ– DeepSpeed engine
            if self.is_main_process:
                print(f"\n" + "=" * 80)
                print("Initializing DeepSpeed")
                print("=" * 80)
                print(f"  DeepSpeed config: {self.deepspeed_config_path}")
                print(f"  Local rank: {self.local_rank}")
                print(f"  World size: {self.world_size}")

            # è¯»å–å¹¶æ›´æ–° DeepSpeed é…ç½®ï¼Œè®¾ç½®å®é™…çš„è®­ç»ƒå‚æ•°
            import json

            with open(self.deepspeed_config_path, "r") as f:
                ds_config = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† ZeRO Stage 3 + CPU Offload
            zero_config = ds_config.get("zero_optimization", {})
            zero_stage = zero_config.get("stage", 0)
            use_cpu_offload = (
                zero_config.get("offload_optimizer", {}).get("device", None) == "cpu"
                or zero_config.get("offload_param", {}).get("device", None) == "cpu"
            )
            
            # å¦‚æœä½¿ç”¨ ZeRO Stage 3 + CPU Offloadï¼Œä¸ä¼ é€’è‡ªå®šä¹‰ optimizer
            # è®© DeepSpeed ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„ä¼˜åŒ–å™¨
            use_custom_optimizer = not (zero_stage == 3 and use_cpu_offload)
            
            if self.is_main_process:
                print(f"  ZeRO Stage: {zero_stage}")
                print(f"  CPU Offload: {use_cpu_offload}")
                print(f"  Use custom optimizer: {use_custom_optimizer}")
            
            # åªæœ‰åœ¨ä¸ä½¿ç”¨ ZeRO Stage 3 + CPU Offload æ—¶æ‰åˆ›å»ºè‡ªå®šä¹‰ä¼˜åŒ–å™¨
            optimizer = None
            lr_scheduler = None
            
            if use_custom_optimizer:
                # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼ˆå¯¹äºLoRAæˆ–stage2è®­ç»ƒå¾ˆé‡è¦ï¼‰
                model_parameters = [p for p in self.model.parameters() if p.requires_grad]
                if len(model_parameters) == 0:
                    raise ValueError(
                        "No trainable parameters found! Check your training configuration. "
                        "For stage2 training, ensure enable_lora=True or train_lm_head_only=True."
                    )
                optimizer = AdamW(
                    model_parameters,
                    lr=config["training"]["learning_rate"],
                    betas=(0.9, 0.999),
                    eps=1e-8,
                    weight_decay=0.01,
                )
                lr_scheduler = get_linear_schedule_with_warmup(
                    optimizer,
                    num_warmup_steps=config["training"]["warmup_steps"],
                    num_training_steps=num_training_steps,
                )
                if self.is_main_process:
                    print(f"  Created custom AdamW optimizer")
            else:
                # éªŒè¯å¯è®­ç»ƒå‚æ•°å­˜åœ¨ï¼ˆå³ä½¿ä¸åˆ›å»ºä¼˜åŒ–å™¨ï¼‰
                model_parameters = [p for p in self.model.parameters() if p.requires_grad]
                if len(model_parameters) == 0:
                    raise ValueError(
                        "No trainable parameters found! Check your training configuration. "
                        "For stage2 training, ensure enable_lora=True, full_finetuning=True, or train_lm_head_only=True."
                    )
                if self.is_main_process:
                    print(f"  Will use DeepSpeed optimizer from config (required for ZeRO Stage 3 + CPU Offload)")
                    print(f"  Trainable parameters: {len(model_parameters)}")

            # è®¾ç½®å®é™…çš„è®­ç»ƒå‚æ•°ï¼ˆè€Œä¸æ˜¯ "auto"ï¼‰
            batch_size = config["training"]["batch_size"]
            gradient_accumulation_steps = config["training"].get("gradient_accumulation_steps", 1)
            learning_rate = config["training"]["learning_rate"]
            warmup_steps = config["training"].get("warmup_steps", 100)
            max_grad_norm = config["training"].get("max_grad_norm", 1.0)

            # è®¡ç®—å…¨å±€ batch size: per_gpu_batch_size * num_gpus * gradient_accumulation_steps
            train_batch_size = batch_size * self.world_size * gradient_accumulation_steps

            ds_config["train_batch_size"] = train_batch_size
            ds_config["train_micro_batch_size_per_gpu"] = batch_size
            ds_config["gradient_accumulation_steps"] = gradient_accumulation_steps
            ds_config["gradient_clipping"] = max_grad_norm

            # è®¾ç½®ä¼˜åŒ–å™¨å­¦ä¹ ç‡
            if "optimizer" in ds_config and "params" in ds_config["optimizer"]:
                ds_config["optimizer"]["params"]["lr"] = learning_rate

            # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥æ•°
            if "scheduler" in ds_config and "params" in ds_config["scheduler"]:
                ds_config["scheduler"]["params"]["warmup_num_steps"] = warmup_steps
                ds_config["scheduler"]["params"]["warmup_min_lr"] = 0
                ds_config["scheduler"]["params"]["warmup_max_lr"] = learning_rate

            if self.is_main_process:
                print(f"  Train batch size: {train_batch_size}")
                print(f"  Micro batch size per GPU: {batch_size}")
                print(f"  Gradient accumulation steps: {gradient_accumulation_steps}")
                print(f"  Learning rate: {learning_rate}")

            # DeepSpeed åˆå§‹åŒ–ï¼šä¼ å…¥æ›´æ–°åçš„é…ç½®
            # DeepSpeed ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°æ­£ç¡®çš„ GPU
            # ä¸´æ—¶æŠ‘åˆ¶ INFO çº§åˆ«çš„æ—¥å¿—ï¼ˆåŒ…æ‹¬ MPI æ£€æµ‹è­¦å‘Šï¼‰
            import logging

            old_levels = {}
            for logger_name in ["deepspeed", "deepspeed.comm", "deepspeed.utils"]:
                logger = logging.getLogger(logger_name)
                old_levels[logger_name] = logger.level
                logger.setLevel(logging.ERROR)  # ä¸´æ—¶è®¾ç½®ä¸º ERROR ä»¥å®Œå…¨æŠ‘åˆ¶ INFO

            try:
                # æ ¹æ®æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨æ¥å†³å®šä¼ é€’å‚æ•°
                init_kwargs = {
                    "model": self.model,
                    "config": ds_config,  # ä¼ å…¥é…ç½®å­—å…¸è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
                }
                
                # åªæœ‰åœ¨ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨æ—¶æ‰ä¼ é€’ optimizer å’Œ lr_scheduler
                if use_custom_optimizer:
                    init_kwargs["optimizer"] = optimizer
                    init_kwargs["lr_scheduler"] = lr_scheduler
                
                self.model_engine, self.optimizer, _, self.scheduler = deepspeed.initialize(**init_kwargs)
            finally:
                # æ¢å¤æ—¥å¿—çº§åˆ«
                for logger_name, old_level in old_levels.items():
                    logging.getLogger(logger_name).setLevel(old_level)

            # é‡æ–°åˆ›å»º DataLoaderï¼ˆDeepSpeed éœ€è¦åˆ†å¸ƒå¼é‡‡æ ·ï¼‰
            from torch.utils.data.distributed import DistributedSampler

            train_sampler = DistributedSampler(
                self.train_dataset,
                num_replicas=self.world_size,
                rank=self.local_rank,
                shuffle=True,
            )
            self.train_sampler = train_sampler  # ä¿å­˜å¼•ç”¨ï¼Œç”¨äºåœ¨æ¯ä¸ª epoch è°ƒç”¨ set_epoch

            self.train_loader = DataLoader(
                self.train_dataset,
                batch_size=config["training"]["batch_size"],
                sampler=train_sampler,
                collate_fn=collate_fn,
                num_workers=config["misc"]["num_workers"],
                pin_memory=config["training"].get("dataloader_pin_memory", True),
            )

            # DeepSpeed ä¼šè‡ªåŠ¨è®¾ç½®æ¨¡å‹åˆ°æ­£ç¡®çš„è®¾å¤‡
            # è·å–å®é™…æ¨¡å‹ï¼ˆDeepSpeed ä¼šåŒ…è£…æ¨¡å‹ï¼‰
            self.model = self.model_engine.module  # è·å–å®é™…æ¨¡å‹

            # ç¡®ä¿æ¨¡å‹çš„ device å±æ€§æ­£ç¡®æ›´æ–°
            if hasattr(self.model, "device"):
                # ä»æ¨¡å‹å‚æ•°ä¸­è·å–å®é™…è®¾å¤‡
                model_device = next(self.model.parameters()).device
                self.model.device = str(model_device)

            # ç¡®ä¿æ‰€æœ‰å­æ¨¡å—éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆåŒ…æ‹¬å‚æ•°å’Œç¼“å†²åŒºï¼‰
            # DeepSpeed å¯èƒ½åªç§»åŠ¨äº†éƒ¨åˆ†æ¨¡å—ï¼Œéœ€è¦é€’å½’ç¡®ä¿æ‰€æœ‰å­æ¨¡å—éƒ½åœ¨æ­£ç¡®è®¾å¤‡
            model_device = next(self.model.parameters()).device

            def move_module_to_device(module, device):
                """é€’å½’åœ°å°†æ¨¡å—åŠå…¶æ‰€æœ‰å­æ¨¡å—ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
                try:
                    module.to(device)
                except Exception as e:
                    if self.is_main_process:
                        print(f"Warning: Could not move module {type(module).__name__} to {device}: {e}")

                # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                for name, param in module.named_parameters(recurse=False):
                    if param.device != device:
                        try:
                            param.data = param.data.to(device)
                        except Exception as e:
                            if self.is_main_process:
                                print(f"Warning: Could not move parameter {name} to {device}: {e}")

                # ç¡®ä¿æ‰€æœ‰æ³¨å†Œç¼“å†²åŒºéƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼ˆè¿™å¯¹äº rotary_emb ç­‰æ¨¡å—å¾ˆé‡è¦ï¼‰
                for name, buffer in module.named_buffers(recurse=False):
                    if buffer.device != device:
                        try:
                            buffer.data = buffer.data.to(device)
                        except Exception as e:
                            if self.is_main_process:
                                print(f"Warning: Could not move buffer {name} to {device}: {e}")

                # é€’å½’å¤„ç†æ‰€æœ‰å­æ¨¡å—
                for child_name, child_module in module.named_children():
                    move_module_to_device(child_module, device)

            # ç§»åŠ¨ vision_encoder åŠå…¶æ‰€æœ‰å­æ¨¡å—
            if hasattr(self.model, "vision_encoder"):
                move_module_to_device(self.model.vision_encoder, model_device)
                # ç‰¹åˆ«å¤„ç† vision_encoder çš„å­æ¨¡å—
                if hasattr(self.model.vision_encoder, "mllm_model"):
                    move_module_to_device(self.model.vision_encoder.mllm_model, model_device)

            # ç§»åŠ¨ language_model åŠå…¶æ‰€æœ‰å­æ¨¡å—ï¼ˆåŒ…æ‹¬ rotary_emb ç­‰ï¼‰
            # è¿™æ˜¯æœ€é‡è¦çš„ï¼Œå› ä¸º rotary_emb çš„ç¼“å†²åŒºå¿…é¡»åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if hasattr(self.model, "language_model"):
                move_module_to_device(self.model.language_model, model_device)
                # ç‰¹åˆ«æ£€æŸ¥ rotary_emb æ¨¡å—ï¼ˆQwen3-VL å¯èƒ½åœ¨å¤šä¸ªåœ°æ–¹æœ‰ rotary_embï¼‰
                try:
                    # éå†æ‰€æœ‰å­æ¨¡å—ï¼Œæ‰¾åˆ°æ‰€æœ‰åŒ…å« rotary æˆ– rope çš„æ¨¡å—
                    for name, child in self.model.language_model.named_modules():
                        if "rotary" in name.lower() or "rope" in name.lower():
                            move_module_to_device(child, model_device)
                            if self.is_main_process:
                                print(f"  Found and moved {name} to {model_device}")
                except Exception as e:
                    if self.is_main_process:
                        print(f"Warning: Could not find/move rotary_emb modules: {e}")

            # ç§»åŠ¨ projection_head
            if hasattr(self.model, "projection_head") and self.model.projection_head is not None:
                move_module_to_device(self.model.projection_head, model_device)

            if self.is_main_process:
                print("âœ“ DeepSpeed initialized successfully")
                print(f"  Model device: {next(self.model.parameters()).device}")
                if hasattr(self.model, "vision_encoder"):
                    try:
                        vision_device = next(self.model.vision_encoder.parameters()).device
                        print(f"  Vision encoder device: {vision_device}")
                    except:
                        pass
                if hasattr(self.model, "language_model"):
                    try:
                        lm_device = next(self.model.language_model.parameters()).device
                        print(f"  Language model device: {lm_device}")
                        # æ£€æŸ¥ rotary_emb çš„è®¾å¤‡
                        if hasattr(self.model.language_model, "model") and hasattr(
                            self.model.language_model.model, "embed_tokens"
                        ):
                            if hasattr(self.model.language_model.model.embed_tokens, "rotary_emb"):
                                try:
                                    rotary_device = next(
                                        self.model.language_model.model.embed_tokens.rotary_emb.parameters()
                                    ).device
                                    print(f"  Rotary embedding device: {rotary_device}")
                                except:
                                    pass
                    except:
                        pass
        else:
            # æ ‡å‡† PyTorch è®­ç»ƒ
            # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
            trainable_params = [p for p in self.model.parameters() if p.requires_grad]
            if len(trainable_params) == 0:
                raise ValueError(
                    "No trainable parameters found! Check your training configuration. "
                    "For stage2 training, ensure enable_lora=True or train_lm_head_only=True."
                )
            self.optimizer = AdamW(
                trainable_params,
                lr=config["training"]["learning_rate"],
            )

            num_training_steps = len(self.train_loader) * config["training"]["num_epochs"]
            self.scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=config["training"]["warmup_steps"],
                num_training_steps=num_training_steps,
            )

            # å¦‚æœä½¿ç”¨å¤šå¡ DDPï¼ˆé DeepSpeedï¼‰
            if self.local_rank >= 0:
                self.model = torch.nn.parallel.DistributedDataParallel(
                    self.model,
                    device_ids=[self.local_rank],
                    output_device=self.local_rank,
                    find_unused_parameters=False,
                )

            self.model_engine = None

        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_loss = float("inf")
        
        # ä¿å­˜ sampler å¼•ç”¨ï¼ˆç”¨äºåœ¨æ¯ä¸ª epoch è°ƒç”¨ set_epochï¼‰
        self.train_sampler = None

        # ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼šåŠ è½½ç¬¬ä¸€é˜¶æ®µçš„æ£€æŸ¥ç‚¹
        if config["training"].get("stage2_mode", False):
            stage1_checkpoint = config["logging"].get("stage1_checkpoint", None)
            if stage1_checkpoint and os.path.exists(stage1_checkpoint):
                if self.is_main_process:
                    print(f"\n" + "="*80)
                    print("Loading Stage 1 Checkpoint")
                    print("="*80)
                    print(f"  Checkpoint path: {stage1_checkpoint}")
                
                # åªåŠ è½½ projection_head çš„æƒé‡ï¼ˆç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„æ¨¡å—ï¼‰
                self._load_stage1_checkpoint(stage1_checkpoint)
                
                if self.is_main_process:
                    print("âœ“ Stage 1 checkpoint loaded successfully")
                    print("="*80)

        if self.is_main_process:
            print(f"\nâœ“ Training setup complete", flush=True)
            print(f"  Device: {self.device}", flush=True)
            print(f"  Local rank: {self.local_rank}", flush=True)
            print(f"  World size: {self.world_size}", flush=True)
            print(f"  Use DeepSpeed: {self.use_deepspeed}", flush=True)
            print(f"  Training samples: {len(self.train_dataset)}", flush=True)
            print(f"  Batch size: {config['training']['batch_size']}", flush=True)
            print(f"  Total steps: {num_training_steps}", flush=True)
            
            # éªŒè¯æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶
            print(f"\nğŸ“ Logging Configuration:", flush=True)
            print(f"  Log directory: {self.log_dir.absolute()}", flush=True)
            print(f"  TensorBoard dir: {self.tensorboard_dir.absolute() if hasattr(self, 'tensorboard_dir') and self.tensorboard_dir else 'Not initialized'}", flush=True)
            
            # æ£€æŸ¥ JSONL æ—¥å¿—æ–‡ä»¶
            jsonl_file = self.log_dir / "training_log.jsonl"
            if jsonl_file.exists():
                print(f"  JSONL log file: {jsonl_file.absolute()} (exists)", flush=True)
            else:
                print(f"  JSONL log file: {jsonl_file.absolute()} (will be created)", flush=True)
            
            # æ£€æŸ¥ TensorBoard æ–‡ä»¶
            if hasattr(self, 'tensorboard_dir') and self.tensorboard_dir.exists():
                tb_files = list(self.tensorboard_dir.glob("events.out.tfevents.*"))
                if tb_files:
                    print(f"  TensorBoard files: {len(tb_files)} file(s) found", flush=True)
                else:
                    print(f"  TensorBoard files: Will be created during training", flush=True)
            print("", flush=True)

    def train_epoch(self, epoch: int, skip_batches: int = 0) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ª epoch

        Args:
            epoch: å½“å‰ epoch ç´¢å¼•
            skip_batches: è·³è¿‡çš„ batch æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰

        Returns:
            åŒ…å«å¹³å‡æŸå¤±çš„å­—å…¸ {'total_loss', 'vision_loss', 'lm_loss'}
        """
        if self.model_engine is not None:
            self.model_engine.train()
        else:
            self.model.train()
        epoch_losses = []
        epoch_vision_losses = []
        epoch_lm_losses = []

        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if self.is_main_process:
            progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")
        else:
            progress_bar = self.train_loader

        # è®°å½•è·³è¿‡çš„ batch
        skipped_count = 0

        for batch_idx, batch in enumerate(progress_bar):
            # è·³è¿‡å·²ç»è®­ç»ƒè¿‡çš„ batch
            if skipped_count < skip_batches:
                skipped_count += 1
                continue

            # æ‰¹é‡å¤„ç†
            questions = batch["questions"]
            cots = batch["cots"]
            answers = batch["answers"]

            # å‰å‘ä¼ æ’­ï¼ˆæ‰¹é‡ï¼‰
            if self.model_engine is not None:
                # DeepSpeed æ¨¡å¼ä¸‹
                outputs = self.model_engine(
                    question_texts=questions, cot_texts=cots, answer_texts=answers, return_loss=True
                )
            else:
                outputs = self.model(question_texts=questions, cot_texts=cots, answer_texts=answers, return_loss=True)

            loss = outputs["loss"]
            vision_loss = outputs.get("vision_loss", None)
            lm_loss = outputs.get("lm_loss", None)

            if loss is not None:
                # DeepSpeed è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯å’Œåå‘ä¼ æ’­
                if self.model_engine is not None:
                    self.model_engine.backward(loss)
                    self.model_engine.step()
                else:
                    # æ ‡å‡† PyTorch è®­ç»ƒ
                    loss = loss / self.config["training"]["gradient_accumulation_steps"]
                    loss.backward()

                # åªä¿ç•™lossçš„æ•°å€¼ï¼Œä¸ä¿ç•™tensorå¼•ç”¨
                loss_value = loss.item()
                if self.model_engine is None:
                    loss_value = loss_value * self.config["training"]["gradient_accumulation_steps"]
                epoch_losses.append(loss_value)

                # è®°å½•åˆ†é¡¹loss
                if vision_loss is not None:
                    epoch_vision_losses.append(vision_loss.item())
                if lm_loss is not None:
                    epoch_lm_losses.append(lm_loss.item())

            # æ¸…ç†outputså­—å…¸ï¼Œé‡Šæ”¾æ˜¾å­˜
            del outputs, loss, vision_loss, lm_loss
            # å®šæœŸæ¸…ç©ºlosså†å²ï¼Œåªä¿ç•™æœ€è¿‘100ä¸ª
            if len(epoch_losses) > 100:
                epoch_losses = epoch_losses[-100:]
                epoch_vision_losses = epoch_vision_losses[-100:]
                epoch_lm_losses = epoch_lm_losses[-100:]

            # æ¢¯åº¦ç´¯ç§¯ï¼ˆä»…é DeepSpeed æ¨¡å¼ï¼ŒDeepSpeed ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            if self.model_engine is None:
                if (batch_idx + 1) % self.config["training"]["gradient_accumulation_steps"] == 0:
                    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config["training"]["max_grad_norm"])

                    # æ›´æ–°å‚æ•°
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad(set_to_none=True)  # ä½¿ç”¨set_to_none=Trueæ›´å½»åº•åœ°æ¸…ç†æ¢¯åº¦

                    self.global_step += 1

                    # è®¡ç®—å½“å‰æŸå¤±ï¼ˆç”¨äºè®°å½•ï¼‰
                    current_loss = epoch_losses[-1] if epoch_losses else 0.0
                    current_vision_loss = epoch_vision_losses[-1] if epoch_vision_losses else 0.0
                    current_lm_loss = epoch_lm_losses[-1] if epoch_lm_losses else 0.0

                    # æ¯ä¸ª step éƒ½è®°å½•åˆ° TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
                    if self.is_main_process and self.writer is not None:
                        try:
                            self.writer.add_scalar("Step/total_loss", current_loss, self.global_step)
                            self.writer.add_scalar("Step/vision_loss", current_vision_loss, self.global_step)
                            self.writer.add_scalar("Step/lm_loss", current_lm_loss, self.global_step)
                            self.writer.add_scalar("Step/learning_rate", self.scheduler.get_last_lr()[0], self.global_step)
                            # å®šæœŸåˆ·æ–° TensorBoardï¼ˆæ¯10ä¸ªstepï¼‰
                            if self.global_step % 10 == 0:
                                self.writer.flush()
                        except Exception as e:
                            if self.global_step % 100 == 0:  # æ¯100æ­¥åªæ‰“å°ä¸€æ¬¡è­¦å‘Šï¼Œé¿å…åˆ·å±
                                print(f"âš ï¸  Warning: Failed to write to TensorBoard: {e}", flush=True)
                    
                    # å®šæœŸæ‰“å°æ—¥å¿—å’Œè®°å½•æ–‡ä»¶ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰- å•å¡è®­ç»ƒæ¨¡å¼
                    if self.is_main_process and self.global_step % self.config["logging"]["log_interval"] == 0:
                        avg_loss = np.mean(epoch_losses[-10:]) if epoch_losses else 0.0
                        avg_vision_loss = np.mean(epoch_vision_losses[-10:]) if epoch_vision_losses else 0.0
                        avg_lm_loss = np.mean(epoch_lm_losses[-10:]) if epoch_lm_losses else 0.0

                        if isinstance(progress_bar, tqdm):
                            current_lr = self.scheduler.get_last_lr()[0]
                            progress_bar.set_postfix(
                                {
                                    "loss": f"{avg_loss:.4f}",
                                    "v_loss": f"{avg_vision_loss:.4f}",
                                    "lm_loss": f"{avg_lm_loss:.4f}",
                                    "lr": f"{current_lr:.2e}",
                                }
                            )

                        # è®°å½•åˆ° JSONL æ–‡ä»¶
                        current_lr = self.scheduler.get_last_lr()[0]
                        self._log_metrics(
                            {
                                "loss": avg_loss,
                                "vision_loss": avg_vision_loss,
                                "lm_loss": avg_lm_loss,
                                "lr": current_lr,
                            }
                        )

                        # é¢å¤–è®°å½•å¹³å‡æŸå¤±åˆ° TensorBoardï¼ˆç”¨äºå¹³æ»‘æ›²çº¿ï¼‰
                        if self.writer is not None:
                            try:
                                self.writer.add_scalar("Average/total_loss", avg_loss, self.global_step)
                                self.writer.add_scalar("Average/vision_loss", avg_vision_loss, self.global_step)
                                self.writer.add_scalar("Average/lm_loss", avg_lm_loss, self.global_step)
                                self.writer.flush()  # ç«‹å³åˆ·æ–°
                            except Exception as e:
                                print(f"âš ï¸  Warning: Failed to write average metrics to TensorBoard: {e}", flush=True)
                    
                    # å®šæœŸæ¸…ç†æ˜¾å­˜
                    if self.global_step % 10 == 0:
                        torch.cuda.empty_cache()
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæŒ‰ step é—´éš”ï¼‰- å•å¡è®­ç»ƒæ¨¡å¼
                    should_save = self.global_step % self.config["logging"]["save_interval"] == 0
                    if should_save:
                        if self.is_main_process:
                            print(f"\n[Step {self.global_step}] Saving checkpoint...", flush=True)
                        self.save_checkpoint(f"checkpoint_step_{self.global_step}")
            else:
                # DeepSpeed æ¨¡å¼ï¼šæ¯æ¬¡ backward å’Œ step éƒ½ä¼šæ›´æ–°ï¼ˆDeepSpeed è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯ï¼‰
                self.global_step += 1

                # è®¡ç®—å½“å‰æŸå¤±ï¼ˆç”¨äºè®°å½•ï¼‰
                current_loss = epoch_losses[-1] if epoch_losses else 0.0
                current_vision_loss = epoch_vision_losses[-1] if epoch_vision_losses else 0.0
                current_lm_loss = epoch_lm_losses[-1] if epoch_lm_losses else 0.0

                # æ¯ä¸ª step éƒ½è®°å½•åˆ° TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
                if self.is_main_process and self.writer is not None:
                    current_lr = (
                        self.scheduler.get_last_lr()[0]
                        if hasattr(self.scheduler, "get_last_lr")
                        else self.scheduler.get_lr()[0]
                    )
                    self.writer.add_scalar("Step/total_loss", current_loss, self.global_step)
                    self.writer.add_scalar("Step/vision_loss", current_vision_loss, self.global_step)
                    self.writer.add_scalar("Step/lm_loss", current_lm_loss, self.global_step)
                    self.writer.add_scalar("Step/learning_rate", current_lr, self.global_step)

                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if self.global_step % 10 == 0:
                    torch.cuda.empty_cache()

                # å®šæœŸæ‰“å°æ—¥å¿—å’Œè®°å½•æ–‡ä»¶ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
                if self.is_main_process and self.global_step % self.config["logging"]["log_interval"] == 0:
                    avg_loss = np.mean(epoch_losses[-10:]) if epoch_losses else 0.0
                    avg_vision_loss = np.mean(epoch_vision_losses[-10:]) if epoch_vision_losses else 0.0
                    avg_lm_loss = np.mean(epoch_lm_losses[-10:]) if epoch_lm_losses else 0.0

                    if isinstance(progress_bar, tqdm):
                        current_lr = (
                            self.scheduler.get_last_lr()[0]
                            if hasattr(self.scheduler, "get_last_lr")
                            else self.scheduler.get_lr()[0]
                        )
                        progress_bar.set_postfix(
                            {
                                "loss": f"{avg_loss:.4f}",
                                "v_loss": f"{avg_vision_loss:.4f}",
                                "lm_loss": f"{avg_lm_loss:.4f}",
                                "lr": f"{current_lr:.2e}",
                            }
                        )

                    # è®°å½•åˆ°æ–‡ä»¶
                    current_lr = (
                        self.scheduler.get_last_lr()[0]
                        if hasattr(self.scheduler, "get_last_lr")
                        else self.scheduler.get_lr()[0]
                    )
                    self._log_metrics(
                        {
                            "loss": avg_loss,
                            "vision_loss": avg_vision_loss,
                            "lm_loss": avg_lm_loss,
                            "lr": current_lr,
                        }
                    )

                    # é¢å¤–è®°å½•å¹³å‡æŸå¤±åˆ° TensorBoardï¼ˆç”¨äºå¹³æ»‘æ›²çº¿ï¼‰
                    if self.writer is not None:
                        self.writer.add_scalar("Average/total_loss", avg_loss, self.global_step)
                        self.writer.add_scalar("Average/vision_loss", avg_vision_loss, self.global_step)
                        self.writer.add_scalar("Average/lm_loss", avg_lm_loss, self.global_step)

                # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæŒ‰ step é—´éš”ï¼‰
                # DeepSpeed æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦å‚ä¸ä¿å­˜ï¼Œä½†åªåœ¨æŒ‡å®šé—´éš”æ—¶è§¦å‘
                should_save = self.global_step % self.config["logging"]["save_interval"] == 0
                if should_save:
                    if self.is_main_process:
                        print(f"\n[Step {self.global_step}] Saving checkpoint...")
                    self.save_checkpoint(f"checkpoint_step_{self.global_step}")

        # è®¡ç®— epoch å¹³å‡æŸå¤±
        avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else float("inf")
        avg_epoch_vision_loss = np.mean(epoch_vision_losses) if epoch_vision_losses else 0.0
        avg_epoch_lm_loss = np.mean(epoch_lm_losses) if epoch_lm_losses else 0.0

        return {
            "total_loss": avg_epoch_loss,
            "vision_loss": avg_epoch_vision_loss,
            "lm_loss": avg_epoch_lm_loss,
        }

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        if self.is_main_process:
            print("\n" + "=" * 80)
            print("Starting Training")
            print("=" * 80)

        # è®¡ç®—æ–­ç‚¹ç»­è®­çš„èµ·å§‹ä½ç½®
        start_epoch = 0
        skip_batches_first_epoch = 0
        
        if self.global_step > 0:
            # è®¡ç®—å·²ç»å¤„ç†è¿‡çš„ micro-batches æ€»æ•°
            if self.use_deepspeed:
                # åœ¨æœ¬è„šæœ¬ä¸­ï¼ŒDeepSpeed æ¨¡å¼ä¸‹çš„ global_step å®é™…ä¸Šæ˜¯ micro-batches è®¡æ•°
                batches_processed = self.global_step
            else:
                # é DeepSpeed æ¨¡å¼ï¼Œglobal_step æ˜¯ä¼˜åŒ–å™¨æ­¥æ•°
                batches_processed = self.global_step * self.config["training"].get("gradient_accumulation_steps", 1)
            
            # è®¡ç®—æ¯ä¸ª epoch çš„ batch æ•°
            batches_per_epoch = len(self.train_loader)
            
            start_epoch = batches_processed // batches_per_epoch
            skip_batches_first_epoch = batches_processed % batches_per_epoch
            
            if self.is_main_process:
                print(f"\nResume training info:")
                print(f"  Global step: {self.global_step}")
                print(f"  Batches processed: {batches_processed}")
                print(f"  Start epoch: {start_epoch + 1}")
                print(f"  Skip batches in first epoch: {skip_batches_first_epoch}")

        for epoch in range(start_epoch, self.config["training"]["num_epochs"]):
            # é‡è¦ï¼šåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨ set_epoch
            # è¿™å¯¹äº DistributedSampler è‡³å…³é‡è¦ï¼Œç¡®ä¿æ¯ä¸ª epoch çš„æ•°æ®åˆ†å¸ƒä¸åŒ
            if self.train_sampler is not None:
                self.train_sampler.set_epoch(epoch)
            
            if self.is_main_process:
                print(f"\n{'='*80}")
                print(f"Epoch {epoch+1}/{self.config['training']['num_epochs']}")
                print(f"{'='*80}")

            # è®¡ç®—å½“å‰ epoch éœ€è¦è·³è¿‡çš„ batch æ•°
            current_skip = skip_batches_first_epoch if epoch == start_epoch else 0
            if current_skip > 0 and self.is_main_process:
                print(f"  Skipping first {current_skip} batches...")
                
            epoch_metrics = self.train_epoch(epoch, skip_batches=current_skip)

            epoch_total_loss = epoch_metrics["total_loss"]
            epoch_vision_loss = epoch_metrics["vision_loss"]
            epoch_lm_loss = epoch_metrics["lm_loss"]

            # é‡è¦ï¼šåœ¨ DeepSpeed æ¨¡å¼ä¸‹ï¼Œå…ˆåŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆ epoch è®­ç»ƒ
            if self.use_deepspeed:
                torch.distributed.barrier()
                # æ¸…ç†æœªå®Œæˆçš„ CUDA æ“ä½œï¼Œé¿å…é˜»å¡åç»­çš„ NCCL é€šä¿¡
                torch.cuda.synchronize()

            if self.is_main_process:
                print(f"\nEpoch {epoch+1} completed:")
                print(f"  Total Loss: {epoch_total_loss:.4f}")
                print(f"  Vision Loss: {epoch_vision_loss:.6f}")
                print(f"  LM Loss: {epoch_lm_loss:.4f}")

                # è®°å½• epoch çº§åˆ«çš„æŸå¤±åˆ° TensorBoard
                if self.writer is not None:
                    self.writer.add_scalar("Epoch/total_loss", epoch_total_loss, epoch + 1)
                    self.writer.add_scalar("Epoch/vision_loss", epoch_vision_loss, epoch + 1)
                    self.writer.add_scalar("Epoch/lm_loss", epoch_lm_loss, epoch + 1)

            # é‡è¦ï¼šåœ¨ DeepSpeed æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦å‚ä¸ä¿å­˜æ£€æŸ¥ç‚¹
            # å› ä¸º ZeRO Stage 2 éœ€è¦èšåˆåˆ†å¸ƒåœ¨å„ä¸ªè¿›ç¨‹ä¸Šçš„å‚æ•°åˆ†ç‰‡
            # ä¿å­˜å‰å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å‡†å¤‡å¥½
            if self.use_deepspeed:
                torch.distributed.barrier()
            
            # ä¿å­˜ epoch æ£€æŸ¥ç‚¹ï¼ˆDeepSpeed æ¨¡å¼ä¸‹æ‰€æœ‰è¿›ç¨‹éƒ½ä¼šè°ƒç”¨ï¼Œä½†åªæœ‰ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼‰
            self.save_checkpoint(f"checkpoint_epoch_{epoch+1}")

            # æ›´æ–°æœ€ä½³æŸå¤±å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if self.use_deepspeed:
                # åœ¨ DeepSpeed æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ä¸»è¿›ç¨‹çš„æŸå¤±å€¼æ¥åˆ¤æ–­ï¼Œä½†æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦å‚ä¸ä¿å­˜
                # ä½¿ç”¨ broadcast è®©æ‰€æœ‰è¿›ç¨‹çŸ¥é“æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä½³æ¨¡å‹
                if self.is_main_process:
                    # ä¸»è¿›ç¨‹åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æœ€ä½³æ¨¡å‹
                    should_save_best = epoch_total_loss < self.best_loss
                    save_flag = torch.tensor(1.0 if should_save_best else 0.0, device=self.device)
                else:
                    save_flag = torch.tensor(0.0, device=self.device)
                
                # å¹¿æ’­ä¿å­˜æ ‡å¿—ï¼Œè®©æ‰€æœ‰è¿›ç¨‹çŸ¥é“æ˜¯å¦éœ€è¦ä¿å­˜
                torch.distributed.broadcast(save_flag, src=0)
                
                if save_flag.item() > 0.5:
                    # ğŸ”§ ä¿®å¤ï¼šæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ›´æ–° best_lossï¼Œç¡®ä¿åŒæ­¥
                    # ä¸»è¿›ç¨‹å…ˆæ›´æ–°ï¼Œç„¶åå¹¿æ’­ç»™æ‰€æœ‰è¿›ç¨‹
                    if self.is_main_process:
                        self.best_loss = epoch_total_loss
                        best_loss_tensor = torch.tensor(self.best_loss, device=self.device)
                    else:
                        best_loss_tensor = torch.tensor(0.0, device=self.device)
                    
                    # å¹¿æ’­ best_loss åˆ°æ‰€æœ‰è¿›ç¨‹
                    torch.distributed.broadcast(best_loss_tensor, src=0)
                    
                    # éä¸»è¿›ç¨‹ä¹Ÿæ›´æ–° best_loss
                    if not self.is_main_process:
                        self.best_loss = best_loss_tensor.item()
                    
                    # æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦å‚ä¸ä¿å­˜ï¼ˆDeepSpeed ZeRO Stage 2 è¦æ±‚ï¼‰
                    torch.distributed.barrier()
                    self.save_checkpoint("best_model")
                    if self.is_main_process:
                        print(f"âœ“ Saved best model (loss: {epoch_total_loss:.4f})")
            else:
                # é DeepSpeed æ¨¡å¼ï¼šåªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
                if epoch_total_loss < self.best_loss:
                    self.best_loss = epoch_total_loss
                    if self.is_main_process:
                        self.save_checkpoint("best_model")
                        print(f"âœ“ Saved best model (loss: {self.best_loss:.4f})")

            # ä¿å­˜ååŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿ä¿å­˜å®Œæˆåå†è¿›å…¥ä¸‹ä¸€ä¸ª epoch
            if self.use_deepspeed:
                torch.distributed.barrier()

        # å…³é—­ TensorBoard writerï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            if self.writer is not None:
                self.writer.close()
                print(f"\nâœ“ TensorBoard writer closed", flush=True)

            print("\n" + "=" * 80, flush=True)
            print("Training Completed!", flush=True)
            print("=" * 80, flush=True)
            print(f"Best loss: {self.best_loss:.4f}", flush=True)
            
            # éªŒè¯æ—¥å¿—æ–‡ä»¶
            print(f"\nğŸ“Š Logging Summary:", flush=True)
            if hasattr(self, 'tensorboard_dir') and self.tensorboard_dir:
                print(f"  TensorBoard logs: {self.tensorboard_dir.absolute()}", flush=True)
                tb_files = list(self.tensorboard_dir.glob("events.out.tfevents.*"))
                print(f"  TensorBoard files: {len(tb_files)} file(s)", flush=True)
                if tb_files:
                    print(f"  To view: tensorboard --logdir={self.tensorboard_dir}", flush=True)
            
            jsonl_file = self.log_dir / "training_log.jsonl"
            if jsonl_file.exists():
                file_size = jsonl_file.stat().st_size
                try:
                    line_count = sum(1 for _ in open(jsonl_file, encoding='utf-8'))
                except:
                    line_count = 0
                print(f"  JSONL log file: {jsonl_file.absolute()}", flush=True)
                print(f"  JSONL size: {file_size:,} bytes, {line_count} lines", flush=True)
            else:
                print(f"  âš ï¸  JSONL log file not found: {jsonl_file.absolute()}", flush=True)
            
            print("=" * 80 + "\n", flush=True)

        # DeepSpeed éœ€è¦åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.use_deepspeed:
            torch.distributed.barrier()

    def save_checkpoint(self, name: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹ä¸º HuggingFace æ ¼å¼"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ stage1 è®­ç»ƒæ¨¡å¼
        model_to_check = self.model.module if hasattr(self.model, "module") else self.model
        if self.model_engine is not None:
            model_to_check = self.model_engine.module
        is_stage1 = not getattr(model_to_check, 'stage2_mode', False)
        
        # DeepSpeed æ£€æŸ¥ç‚¹ä¿å­˜
        if self.model_engine is not None:
            # é‡è¦ï¼šDeepSpeed åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦å‚ä¸ä¿å­˜æ£€æŸ¥ç‚¹
            # ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨ ZeRO æ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹ä¿å­˜ä¸åŒçš„åˆ†ç‰‡ï¼‰
            # æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦è°ƒç”¨ model_engine.save_checkpoint()

            output_dir = self.output_dir / name

            # ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•å¹¶æ‰“å°æ—¥å¿—
            if self.is_main_process:
                output_dir.mkdir(parents=True, exist_ok=True)
                if is_stage1:
                    print(f"\nSaving Stage 1 checkpoint (projection_head only): {output_dir}")
                else:
                    print(f"\nSaving DeepSpeed checkpoint: {output_dir}")
                    print("  Note: DeepSpeed checkpoint saving may take a while...")
                    print("  All processes are participating in checkpoint saving...")
                import time

                start_time = time.time()
            else:
                # éä¸»è¿›ç¨‹ï¼šç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆé¿å…ç«æ€æ¡ä»¶ï¼‰
                output_dir.mkdir(parents=True, exist_ok=True)
                import time

                start_time = time.time()

            # é‡è¦ï¼šåœ¨ä¿å­˜æ£€æŸ¥ç‚¹ä¹‹å‰ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å·²å®Œæˆå½“å‰çš„è®¡ç®—å¹¶åŒæ­¥
            # è¿™æœ‰åŠ©äºé¿å…åœ¨ DeepSpeed save_checkpoint å†…éƒ¨çš„å‚æ•°èšåˆæ—¶å‡ºç°åŒæ­¥é—®é¢˜
            if self.use_deepspeed:
                # æ¸…ç†å¯èƒ½æœªå®Œæˆçš„ CUDA æ“ä½œï¼Œé¿å…é˜»å¡åç»­çš„ NCCL é€šä¿¡
                torch.cuda.synchronize()
                # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥ï¼ˆåœ¨ä¿å­˜å‰ï¼Œç¡®ä¿ç›®å½•å·²åˆ›å»ºä¸”æ‰€æœ‰è¿›ç¨‹éƒ½å‡†å¤‡å¥½ï¼‰
                torch.distributed.barrier()

            # Stage 1: ä¿å­˜ projection_head + special_tokens + optimizer/schedulerï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
            if is_stage1:
                try:
                    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ projection_head
                    model_to_save = self.model_engine.module
                    if hasattr(model_to_save, 'projection_head') and model_to_save.projection_head is not None:
                        if self.is_main_process:
                            print("  Saving projection_head.bin (Stage 1 training)...")
                            step_start = time.time()
                        torch.save(model_to_save.projection_head.state_dict(), output_dir / "projection_head.bin")
                        if self.is_main_process:
                            elapsed = time.time() - step_start
                            size_mb = (output_dir / "projection_head.bin").stat().st_size / (1024**2)
                            print(f"  âœ“ Saved projection_head.bin ({size_mb:.2f} MB, took {elapsed:.2f} seconds)")
                    
                    # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¿å­˜ special token embeddingsï¼ˆDeepSpeed æ¨¡å¼ï¼‰
                    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ï¼ˆé¿å…å¤šè¿›ç¨‹åŒæ—¶å†™å…¥å†²çªï¼‰
                    if self.is_main_process:
                        try:
                            special_tokens_state = {}
                            
                            # è·å– embedding table
                            embed_table = model_to_save.language_model.get_input_embeddings()
                            
                            # ä¿å­˜ <img_begin> embeddingï¼ˆä» embedding table ä¸­æå–ï¼‰
                            if hasattr(model_to_save, 'img_begin_token_id'):
                                img_begin_emb = embed_table.weight[model_to_save.img_begin_token_id].data.cpu()
                                special_tokens_state['img_begin_emb'] = img_begin_emb
                                norm = img_begin_emb.norm().item()
                                print(f"  âœ“ Saved <img_begin> embedding (norm={norm:.4f})")
                            
                            # ä¿å­˜ <img_end> embeddingï¼ˆä» embedding table ä¸­æå–ï¼‰
                            if hasattr(model_to_save, 'img_end_token_id'):
                                img_end_emb = embed_table.weight[model_to_save.img_end_token_id].data.cpu()
                                special_tokens_state['img_end_emb'] = img_end_emb
                                norm = img_end_emb.norm().item()
                                print(f"  âœ“ Saved <img_end> embedding (norm={norm:.4f})")
                            
                            # ä¿å­˜ token IDsï¼ˆç”¨äºéªŒè¯ï¼‰
                            if hasattr(model_to_save, 'img_begin_token_id') and hasattr(model_to_save, 'img_end_token_id'):
                                special_tokens_state['img_begin_token_id'] = model_to_save.img_begin_token_id
                                special_tokens_state['img_end_token_id'] = model_to_save.img_end_token_id
                            
                            if special_tokens_state:
                                torch.save(special_tokens_state, output_dir / "special_tokens.bin")
                                size_mb = (output_dir / "special_tokens.bin").stat().st_size / (1024**2)
                                print(f"  âœ“ Saved special token embeddings ({size_mb:.4f} MB)")
                        except Exception as e:
                            print(f"  âš ï¸  Warning: Failed to save special token embeddings: {e}")
                            import traceback
                            traceback.print_exc()
                    
                    # ğŸ’¾ ä¿å­˜ optimizer å’Œ scheduler çŠ¶æ€ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
                    # DeepSpeed æ¨¡å¼ï¼šä¿å­˜å®Œæ•´çš„ DeepSpeed checkpointï¼ˆåŒ…å« optimizer å’Œ schedulerï¼‰
                    if self.is_main_process:
                        print("  Saving optimizer and scheduler state for resume training...")
                    
                    try:
                        # ä¿å­˜è®­ç»ƒçŠ¶æ€åˆ° client_state
                        client_state = {
                            "global_step": self.global_step,
                            "best_loss": self.best_loss,
                        }
                        
                        # è°ƒç”¨ DeepSpeed save_checkpoint æ¥ä¿å­˜ optimizer å’Œ scheduler
                        # è¿™ä¼šåœ¨ output_dir ä¸‹åˆ›å»º global_step_XXX ç›®å½•
                        self.model_engine.save_checkpoint(str(output_dir), tag=None, client_state=client_state)
                        
                        if self.is_main_process:
                            print(f"  âœ“ Saved optimizer and scheduler state (DeepSpeed checkpoint)")
                    except Exception as e:
                        if self.is_main_process:
                            print(f"  âš ï¸  Warning: Failed to save optimizer/scheduler state: {e}")
                            print(f"     Resume training will use fresh optimizer/scheduler")
                    
                    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                    if self.use_deepspeed:
                        torch.cuda.synchronize()
                        torch.distributed.barrier()
                    
                    if self.is_main_process:
                        elapsed = time.time() - start_time
                        print(f"âœ“ Saved Stage 1 checkpoint to {output_dir} (took {elapsed:.2f} seconds)")
                        print(f"  Components: projection_head + special tokens + optimizer/scheduler")
                except Exception as e:
                    if self.is_main_process:
                        print(f"âŒ Failed to save Stage 1 checkpoint: {e}")
                        import traceback
                        traceback.print_exc()
                    raise
            else:
                # Stage 2: ä¿å­˜å®Œæ•´çš„ DeepSpeed checkpoint
                # DeepSpeed ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½ä¼šè°ƒç”¨ï¼ŒDeepSpeed å†…éƒ¨ä¼šå¤„ç†åŒæ­¥ï¼‰
                try:
                    if self.is_main_process:
                        print(f"  Process {self.local_rank}: Calling model_engine.save_checkpoint()...")
                        print(f"  This may take a while due to parameter aggregation (ZeRO Stage 2)...")
                        print(f"  Note: If this times out, try increasing NCCL_TIMEOUT environment variable")

                    # é‡è¦ï¼šåœ¨ä½¿ç”¨ ZeRO æ—¶ï¼Œsave_checkpoint ä¼šè§¦å‘å‚æ•°èšåˆï¼ˆALLREDUCEï¼‰
                    # è¿™éœ€è¦æ‰€æœ‰ GPU åŒæ­¥ï¼Œç¡®ä¿åœ¨è°ƒç”¨å‰æ‰€æœ‰è¿›ç¨‹éƒ½å·²å‡†å¤‡å¥½
                    # ä¿å­˜è®­ç»ƒçŠ¶æ€åˆ° client_stateï¼Œä»¥ä¾¿æ¢å¤è®­ç»ƒ
                    client_state = {
                        "global_step": self.global_step,
                        "best_loss": self.best_loss,
                    }
                    
                    # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¿å­˜ training_state.json åˆ°æ ¹ç›®å½•ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
                    # å³ä½¿ DeepSpeed ä¿å­˜å¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿å­˜è¿™ä¸ªæ–‡ä»¶ï¼Œä»¥ä¾¿çŸ¥é“è¿›åº¦
                    if self.is_main_process:
                        try:
                            training_state = {
                                "global_step": self.global_step,
                                "best_loss": self.best_loss,
                                "stage": "stage2",
                                "use_lora": self.config["training"].get("enable_lora", True),
                                "full_finetuning": self.config["training"].get("full_finetuning", False)
                            }
                            # ä¿å­˜åˆ° checkpoint æ ¹ç›®å½•
                            with open(output_dir / "training_state.json", "w") as f:
                                json.dump(training_state, f, indent=2)
                            print(f"  âœ“ Saved training_state.json (step={self.global_step}, best_loss={self.best_loss:.4f})")
                        except Exception as e:
                            print(f"  âš ï¸  Warning: Failed to save training_state.json: {e}")

                    # è°ƒç”¨ DeepSpeed save_checkpoint
                    # æ³¨æ„ï¼šåœ¨ä½¿ç”¨ ZeRO Stage 2/3 æ—¶ï¼Œè¿™ä¼šè§¦å‘å‚æ•°èšåˆï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´
                    # å¦‚æœç½‘ç»œæ–‡ä»¶ç³»ç»Ÿè¾ƒæ…¢ï¼Œå¯èƒ½ä¼šå¢åŠ ç­‰å¾…æ—¶é—´
                    # å¦‚æœå‡ºç° NCCL è¶…æ—¶ï¼Œå»ºè®®ï¼š
                    #   1. å¢åŠ  NCCL_TIMEOUT ç¯å¢ƒå˜é‡ï¼ˆä¾‹å¦‚ï¼šexport NCCL_TIMEOUT=1800ï¼‰
                    #   2. æ£€æŸ¥ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿæ€§èƒ½ï¼ˆå¦‚æœ checkpoint ä¿å­˜åœ¨ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿä¸Šï¼‰
                    self.model_engine.save_checkpoint(str(output_dir), tag=None, client_state=client_state)

                    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆä¿å­˜åå†ç»§ç»­ï¼ˆDeepSpeed save_checkpoint å†…éƒ¨å¯èƒ½å·²ç»åŒæ­¥ï¼Œä½†ä¸ºäº†å®‰å…¨å†æ¬¡åŒæ­¥ï¼‰
                    if self.use_deepspeed:
                        # åœ¨ barrier ä¹‹å‰å†æ¬¡åŒæ­¥ CUDAï¼Œç¡®ä¿æ‰€æœ‰ä¿å­˜æ“ä½œå®Œæˆ
                        torch.cuda.synchronize()
                        torch.distributed.barrier()

                    if self.is_main_process:
                        print(f"  Process {self.local_rank}: save_checkpoint() completed successfully")

                    # åœ¨ DeepSpeed æ£€æŸ¥ç‚¹ä¿å­˜å®Œæˆåï¼Œä¹Ÿä¿å­˜ projection_head.bin
                    # è¿™æ˜¯ä¸ºäº†ä¸å…¶ä»–æ£€æŸ¥ç‚¹æ ¼å¼ä¿æŒä¸€è‡´ï¼Œä¾¿äºåç»­åŠ è½½
                    model_to_save = self.model.module if hasattr(self.model, "module") else self.model
                    if hasattr(model_to_save, 'projection_head') and model_to_save.projection_head is not None:
                        if self.is_main_process:
                            print("  Saving projection_head.bin for compatibility...")
                            step_start = time.time()
                        torch.save(model_to_save.projection_head.state_dict(), output_dir / "projection_head.bin")
                        if self.is_main_process:
                            elapsed = time.time() - step_start
                            size_mb = (output_dir / "projection_head.bin").stat().st_size / (1024**2)
                            print(f"  âœ“ Saved projection_head.bin ({size_mb:.2f} MB, took {elapsed:.2f} seconds)")

                    # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¿å­˜ training_state.json åˆ°æ ¹ç›®å½•ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
                    # è™½ç„¶ client_state è¢«ä¼ é€’ç»™ DeepSpeedï¼Œä½†ä¸ä¿è¯èƒ½æ­£ç¡®è¯»å–
                    # å› æ­¤ä¸»è¿›ç¨‹é¢å¤–ä¿å­˜ training_state.json
                    if self.is_main_process:
                        try:
                            training_state = {
                                "global_step": self.global_step,
                                "best_loss": self.best_loss,
                                "stage": "stage2"
                            }
                            with open(output_dir / "training_state.json", "w") as f:
                                json.dump(training_state, f, indent=2)
                            print(f"  âœ“ Saved training_state.json (step={self.global_step}, best_loss={self.best_loss:.4f})")
                        except Exception as e:
                            print(f"  âš ï¸  Warning: Failed to save training_state.json: {e}")
                    
                    # æ³¨æ„ï¼šæ–‡ä»¶ç³»ç»Ÿç»Ÿè®¡æ“ä½œç§»åˆ° barrier ä¹‹åï¼Œä¸”ä½¿ç”¨éé˜»å¡æ–¹å¼
                    # é¿å…æ–‡ä»¶ç³»ç»Ÿæ“ä½œé˜»å¡å¯¼è‡´åç»­åŒæ­¥è¶…æ—¶
                    if self.is_main_process:
                        elapsed = time.time() - start_time
                        print(f"âœ“ Saved DeepSpeed checkpoint to {output_dir} (took {elapsed:.2f} seconds)")

                        # ç®€åŒ–æ–‡ä»¶ç³»ç»Ÿç»Ÿè®¡ï¼Œé¿å…é€’å½’éå†å¤§é‡æ–‡ä»¶å¯¼è‡´é˜»å¡
                        # åªåœ¨å¿«é€Ÿæ“ä½œå¤±è´¥æ—¶æ‰è·³è¿‡ï¼Œä¸é˜»å¡ä¸»è¿›ç¨‹
                        try:
                            # åªæ£€æŸ¥ç›´æ¥å­ç›®å½•å’Œæ–‡ä»¶ï¼Œä¸é€’å½’ï¼ˆé¿å…ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿé˜»å¡ï¼‰
                            all_items = list(output_dir.iterdir())
                            print(f"  Checkpoint contains {len(all_items)} items")
                            # åªæ˜¾ç¤ºå‰5ä¸ªé¡¹ç›®ï¼Œé¿å…éå†è¿‡å¤š
                            for item in all_items[:5]:
                                try:
                                    if item.is_dir():
                                        print(f"    - {item.name}/ (directory)")
                                    else:
                                        size_mb = item.stat().st_size / (1024**2)
                                        print(f"    - {item.name} ({size_mb:.2f} MB)")
                                except:
                                    pass  # å¿½ç•¥å•ä¸ªæ–‡ä»¶ç»Ÿè®¡å¤±è´¥
                            if len(all_items) > 5:
                                print(f"    ... and {len(all_items) - 5} more items")
                        except Exception as e:
                            # æ–‡ä»¶ç³»ç»Ÿç»Ÿè®¡å¤±è´¥ä¸å½±å“ä¿å­˜æˆåŠŸ
                            print(f"  Note: Could not check checkpoint details (non-critical): {e}")
                except Exception as e:
                    # åœ¨å¼‚å¸¸æƒ…å†µä¸‹ï¼Œå°è¯•åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œä½†ä¸è¦å› ä¸ºåŒæ­¥å¤±è´¥è€Œæ©ç›–åŸå§‹é”™è¯¯
                    if self.use_deepspeed:
                        try:
                            # å°è¯•åŒæ­¥ï¼Œä½†ä¸è¦é˜»å¡å¤ªä¹…
                            torch.cuda.synchronize()
                            # å°è¯• barrierï¼Œä½†å¦‚æœå¤±è´¥ä¹Ÿä¸å½±å“åŸå§‹é”™è¯¯çš„æŠ¥å‘Š
                            torch.distributed.barrier()
                        except Exception as sync_error:
                            # åŒæ­¥å¤±è´¥ä¸å½±å“åŸå§‹é”™è¯¯çš„æŠ¥å‘Š
                            if self.is_main_process:
                                print(f"  Warning: Barrier failed during error handling: {sync_error}")
                    
                    if self.is_main_process:
                        print(f"âŒ Failed to save DeepSpeed checkpoint: {e}")
                        print(f"  Process {self.local_rank} encountered error during save_checkpoint()")
                        import traceback

                        traceback.print_exc()
                    else:
                        # éä¸»è¿›ç¨‹ä¹Ÿæ‰“å°é”™è¯¯ä¿¡æ¯ï¼ˆè™½ç„¶å¯èƒ½ä¸ä¼šæ˜¾ç¤ºï¼Œä½†æœ‰åŠ©äºè°ƒè¯•ï¼‰
                        print(f"[Process {self.local_rank}] Error in save_checkpoint: {e}", flush=True)
                    
                    # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“ä¿å­˜å¤±è´¥
                    raise

            return

        # é DeepSpeed æ¨¡å¼ï¼šåªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
        if not self.is_main_process:
            return

        # ä½¿ç”¨ HF æ ¼å¼ä¿å­˜ï¼ˆåˆ†ç‰‡ï¼‰
        output_dir = self.output_dir / name
        output_dir.mkdir(parents=True, exist_ok=True)

        # è·å–å®é™…æ¨¡å‹ï¼ˆå¦‚æœæ˜¯ DDPï¼Œéœ€è¦ .moduleï¼‰
        model_to_save = self.model.module if hasattr(self.model, "module") else self.model

        # Stage 1: ä¿å­˜ projection_head + special token embeddings + optimizer/scheduler
        if is_stage1:
            if self.is_main_process:
                print(f"\nSaving Stage 1 checkpoint: {output_dir}")
                import time
                checkpoint_start_time = time.time()

            # ä¿å­˜ projection_head
            if model_to_save.projection_head is not None:
                torch.save(model_to_save.projection_head.state_dict(), output_dir / "projection_head.bin")
                if self.is_main_process:
                    elapsed = time.time() - checkpoint_start_time
                    size_mb = (output_dir / "projection_head.bin").stat().st_size / (1024**2)
                    print(f"  âœ“ Saved projection_head ({size_mb:.2f} MB, took {elapsed:.2f} seconds)")
            else:
                if self.is_main_process:
                    print("âš ï¸  Warning: No projection head to save")
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¿å­˜ special token embeddings
            # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒäº†ä¸‰ä¸ªç»„ä»¶ï¼š
            # 1. projection_headï¼ˆå·²ä¿å­˜ï¼‰
            # 2. <img_begin> embeddingï¼ˆåœ¨ embedding table ä¸­ï¼‰
            # 3. <img_end> embeddingï¼ˆåœ¨ embedding table ä¸­ï¼‰
            try:
                special_tokens_state = {}
                
                # è·å– embedding table
                embed_table = model_to_save.language_model.get_input_embeddings()
                
                # ä¿å­˜ <img_begin> embeddingï¼ˆä» embedding table ä¸­æå–ï¼‰
                if hasattr(model_to_save, 'img_begin_token_id'):
                    img_begin_emb = embed_table.weight[model_to_save.img_begin_token_id].data.cpu()
                    special_tokens_state['img_begin_emb'] = img_begin_emb
                    if self.is_main_process:
                        norm = img_begin_emb.norm().item()
                        print(f"  âœ“ Saved <img_begin> embedding (norm={norm:.4f})")
                
                # ä¿å­˜ <img_end> embeddingï¼ˆä» embedding table ä¸­æå–ï¼‰
                if hasattr(model_to_save, 'img_end_token_id'):
                    img_end_emb = embed_table.weight[model_to_save.img_end_token_id].data.cpu()
                    special_tokens_state['img_end_emb'] = img_end_emb
                    if self.is_main_process:
                        norm = img_end_emb.norm().item()
                        print(f"  âœ“ Saved <img_end> embedding (norm={norm:.4f})")
                
                # ä¿å­˜ token IDsï¼ˆç”¨äºéªŒè¯ï¼‰
                if hasattr(model_to_save, 'img_begin_token_id') and hasattr(model_to_save, 'img_end_token_id'):
                    special_tokens_state['img_begin_token_id'] = model_to_save.img_begin_token_id
                    special_tokens_state['img_end_token_id'] = model_to_save.img_end_token_id
                
                if special_tokens_state:
                    torch.save(special_tokens_state, output_dir / "special_tokens.bin")
                    if self.is_main_process:
                        size_mb = (output_dir / "special_tokens.bin").stat().st_size / (1024**2)
                        print(f"  âœ“ Saved special token embeddings ({size_mb:.2f} MB)")
            except Exception as e:
                if self.is_main_process:
                    print(f"  âš ï¸  Warning: Failed to save special token embeddings: {e}")
                    import traceback
                    traceback.print_exc()

            # ğŸ’¾ ä¿å­˜ optimizer å’Œ scheduler çŠ¶æ€ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
            if self.optimizer is not None and self.scheduler is not None:
                try:
                    optimizer_state = {
                        "optimizer_state_dict": self.optimizer.state_dict(),
                        "scheduler_state_dict": self.scheduler.state_dict(),
                    }
                    torch.save(optimizer_state, output_dir / "optimizer.bin")
                    if self.is_main_process:
                        size_mb = (output_dir / "optimizer.bin").stat().st_size / (1024**2)
                        print(f"  âœ“ Saved optimizer and scheduler ({size_mb:.2f} MB)")
                except Exception as e:
                    if self.is_main_process:
                        print(f"  âš ï¸  Warning: Failed to save optimizer/scheduler: {e}")

            # ä¿å­˜è®­ç»ƒä¿¡æ¯
            training_state = {
                "global_step": self.global_step,
                "best_loss": self.best_loss,
                "stage": "stage1",
            }
            with open(output_dir / "training_state.json", "w") as f:
                json.dump(training_state, f, indent=2)

            # ä¿å­˜é…ç½®æ–‡ä»¶
            with open(output_dir / "config.yaml", "w") as f:
                yaml.dump(self.config, f, default_flow_style=False)

            if self.is_main_process:
                elapsed_total = time.time() - checkpoint_start_time
                print(f"âœ“ Saved Stage 1 checkpoint to {output_dir} (took {elapsed_total:.2f} seconds)")
                print(f"  Components: projection_head + special tokens + optimizer/scheduler")
            
            return

        # Stage 2: ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆLoRA + projection head + å…¶ä»–ç»„ä»¶ï¼‰
        if self.is_main_process:
            print(f"\nSaving checkpoint in HF format: {output_dir}")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† LoRA
        use_lora = self.config["training"].get("enable_lora", True)

        if use_lora:
            # ä½¿ç”¨äº† LoRA è®­ç»ƒï¼Œä¿å­˜ LoRA æƒé‡å’ŒæŠ•å½±å±‚æƒé‡
            if self.is_main_process:
                print("  Detected LoRA training, saving LoRA weights and projection head...")
                import time

                checkpoint_start_time = time.time()

            # 1. ä¿å­˜ LoRA æƒé‡
            try:
                from peft import PeftModel

                if hasattr(model_to_save, "language_model") and isinstance(model_to_save.language_model, PeftModel):
                    if self.is_main_process:
                        print("    Step 1/2: Saving LoRA weights...")
                    model_to_save.language_model.save_pretrained(output_dir)
                    if self.is_main_process:
                        elapsed = time.time() - checkpoint_start_time
                        print(f"    âœ“ Saved LoRA weights to {output_dir} (took {elapsed:.2f} seconds)")
                        # æ£€æŸ¥ LoRA æ–‡ä»¶å¤§å°
                        try:
                            lora_size = sum(f.stat().st_size for f in output_dir.glob("adapter*.bin"))
                            size_mb = lora_size / (1024**2)
                            print(f"      LoRA files size: {size_mb:.2f} MB")
                        except:
                            pass
                else:
                    if self.is_main_process:
                        print("âš ï¸  Warning: LoRA enabled but language_model is not PeftModel")
            except Exception as e:
                if self.is_main_process:
                    print(f"âš ï¸  Failed to save LoRA weights: {e}")
                    print("  Continuing with projection head only...")
                    import traceback

                    traceback.print_exc()

            # 2. ä¿å­˜ projection_head
            if model_to_save.projection_head is not None:
                if self.is_main_process:
                    print("    Step 2/2: Saving projection head...")
                    step_start = time.time()
                torch.save(model_to_save.projection_head.state_dict(), output_dir / "projection_head.bin")
                if self.is_main_process:
                    elapsed = time.time() - step_start
                    size_mb = (output_dir / "projection_head.bin").stat().st_size / (1024**2)
                    print(f"    âœ“ Saved projection_head ({size_mb:.2f} MB, took {elapsed:.2f} seconds)")

        else:
            # æ²¡æœ‰ä½¿ç”¨ LoRA è®­ç»ƒï¼Œåªä¿å­˜æŠ•å½±å±‚æƒé‡
            if self.is_main_process:
                print("  Detected non-LoRA training, saving projection head only...")
                import time

                checkpoint_start_time = time.time()

            # åªä¿å­˜ projection_head
            if model_to_save.projection_head is not None:
                torch.save(model_to_save.projection_head.state_dict(), output_dir / "projection_head.bin")
                if self.is_main_process:
                    elapsed = time.time() - checkpoint_start_time
                    size_mb = (output_dir / "projection_head.bin").stat().st_size / (1024**2)
                    print(f"  âœ“ Saved projection_head ({size_mb:.2f} MB, took {elapsed:.2f} seconds)")
            else:
                if self.is_main_process:
                    print("âš ï¸  Warning: No projection head to save")

        # 3. ä¿å­˜å…¶ä»–è®­ç»ƒä¿¡æ¯
        training_state = {
            "global_step": self.global_step,
            "best_loss": self.best_loss,
            "use_lora": use_lora,  # è®°å½•æ˜¯å¦ä½¿ç”¨äº† LoRA
        }
        with open(output_dir / "training_state.json", "w") as f:
            json.dump(training_state, f, indent=2)

        # 4. ä¿å­˜é…ç½®æ–‡ä»¶
        with open(output_dir / "config.yaml", "w") as f:
            yaml.dump(self.config, f, default_flow_style=False)

        # 5. ä¿å­˜ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼Œç”¨äºæ¢å¤è®­ç»ƒï¼‰
        if self.optimizer is not None and self.scheduler is not None:
            optimizer_state = {
                "optimizer_state_dict": self.optimizer.state_dict(),
                "scheduler_state_dict": self.scheduler.state_dict(),
            }
            torch.save(optimizer_state, output_dir / "optimizer.bin")

        if self.is_main_process:
            print(f"âœ“ Saved checkpoint in HF format to {output_dir}")

        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼ˆä¿ç•™æœ€è¿‘ N ä¸ªï¼‰
        self._cleanup_checkpoints()

    def _cleanup_checkpoints(self):
        """æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼ˆä¿ç•™æœ€è¿‘çš„ N ä¸ªç›®å½•ï¼‰"""
        keep_last_n = self.config["logging"]["keep_last_n_checkpoints"]

        # è·å–æ‰€æœ‰ checkpoint ç›®å½•
        checkpoints = sorted(
            [d for d in self.output_dir.glob("checkpoint_step_*") if d.is_dir()], key=lambda x: x.stat().st_mtime
        )

        # åˆ é™¤æ—§çš„ç›®å½•
        for old_checkpoint_dir in checkpoints[:-keep_last_n]:
            shutil.rmtree(old_checkpoint_dir)
            if self.is_main_process:
                print(f"  Removed old checkpoint: {old_checkpoint_dir.name}")

    def _log_metrics(self, metrics: Dict[str, float]):
        """è®°å½•æŒ‡æ ‡åˆ° JSONL æ–‡ä»¶"""
        if not self.is_main_process:
            return  # åªåœ¨ä¸»è¿›ç¨‹è®°å½•
        
        try:
            log_file = self.log_dir / "training_log.jsonl"
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            log_file.parent.mkdir(parents=True, exist_ok=True)

            log_entry = {"step": self.global_step, **metrics}

            # ä½¿ç”¨è¿½åŠ æ¨¡å¼å†™å…¥ï¼Œå¹¶ç«‹å³åˆ·æ–°
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸå†™å…¥
            if log_file.exists() and log_file.stat().st_size > 0:
                pass  # æ–‡ä»¶æ­£å¸¸
            else:
                print(f"âš ï¸  Warning: Log file {log_file} may not be written correctly", flush=True)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to write to log file: {e}", flush=True)
            import traceback
            traceback.print_exc()
    
    def _verify_stage1_checkpoint(self, checkpoint_path: Path) -> bool:
        """
        éªŒè¯ç¬¬ä¸€é˜¶æ®µæ£€æŸ¥ç‚¹çš„å®Œæ•´æ€§
        
        Returns:
            True if checkpoint is valid, False otherwise
        """
        if not checkpoint_path.exists():
            return False
        
        required_files = {
            "projection_head.bin": "Projection head weights",
            "special_tokens.bin": "Special token embeddings",  # ğŸ”§ æ–°å¢
        }
        
        optional_files = {
            "adapter_config.json": "LoRA configuration",
            "adapter_model.bin": "LoRA weights",
            "training_state.json": "Training state",
        }
        
        if self.is_main_process:
            print(f"\n  Validating Stage 1 Checkpoint Structure:")
        
        all_valid = True
        for filename, description in required_files.items():
            file_path = checkpoint_path / filename
            if file_path.exists():
                file_size = file_path.stat().st_size / (1024 * 1024)  # MB
                if self.is_main_process:
                    print(f"    âœ“ {description}: {filename} ({file_size:.2f} MB)")
            else:
                if self.is_main_process:
                    print(f"    âŒ {description}: {filename} (MISSING - REQUIRED)")
                all_valid = False
        
        for filename, description in optional_files.items():
            file_path = checkpoint_path / filename
            if file_path.exists():
                file_size = file_path.stat().st_size / (1024 * 1024)  # MB
                if self.is_main_process:
                    print(f"    â„¹ï¸  {description}: {filename} ({file_size:.2f} MB)")
        
        return all_valid

    def _load_stage1_checkpoint(self, checkpoint_path: str):
        """
        åŠ è½½ç¬¬ä¸€é˜¶æ®µçš„æ£€æŸ¥ç‚¹
        
        ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„ç»„ä»¶ï¼š
        - projection_head: å¿…é¡»åŠ è½½
        - LoRA weights (å¦‚æœä½¿ç”¨): å¯é€‰åŠ è½½
        
        æ³¨æ„ï¼šåœ¨ DeepSpeed æ¨¡å¼ä¸‹ï¼Œéœ€è¦é€šè¿‡ model_engine.module è®¿é—®æ¨¡å‹
        """
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            if self.is_main_process:
                print(f"âš ï¸  Stage 1 checkpoint not found: {checkpoint_path}")
            return
        
        if self.is_main_process:
            print(f"\n{'='*60}")
            print("Loading Stage 1 Model Components")
            print(f"{'='*60}")
        
        # éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
        is_valid = self._verify_stage1_checkpoint(checkpoint_path)
        if not is_valid:
            if self.is_main_process:
                print(f"\nâŒ Stage 1 checkpoint validation failed!")
                print(f"   Please check that the checkpoint is complete.")
            # ä»ç„¶å°è¯•åŠ è½½ï¼Œä½†ä¼šåœ¨åé¢æŠ¥é”™
        else:
            if self.is_main_process:
                print(f"\nâœ“ Stage 1 checkpoint validation passed")
        
        # è·å–å®é™…æ¨¡å‹ï¼ˆå¤„ç† DeepSpeed åŒ…è£…ï¼‰
        if self.use_deepspeed and self.model_engine is not None:
            model = self.model_engine.module
        else:
            model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # 1. åŠ è½½ projection_headï¼ˆå¿…é¡»ï¼‰
        projection_file = checkpoint_path / "projection_head.bin"
        if projection_file.exists():
            if model.projection_head is not None:
                try:
                    state_dict = torch.load(projection_file, map_location='cpu')
                    
                    # ä½¿ç”¨ strict=False ä»¥è·å– missing å’Œ unexpected keys
                    incompatible_keys = model.projection_head.load_state_dict(state_dict, strict=False)
                    
                    # ç¡®ä¿åŠ è½½å projection_head åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    if self.use_deepspeed:
                        # DeepSpeed ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
                        pass
                    else:
                        model.projection_head = model.projection_head.to(self.device)
                    
                    if self.is_main_process:
                        param_count = sum(p.numel() for p in model.projection_head.parameters())
                        print(f"  âœ“ Loaded projection_head ({param_count:,} parameters)")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ missing æˆ– unexpected keys
                        if incompatible_keys.missing_keys:
                            print(f"    âš ï¸  Warning: {len(incompatible_keys.missing_keys)} missing keys in projection_head:")
                            for key in incompatible_keys.missing_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                print(f"       - {key}")
                            if len(incompatible_keys.missing_keys) > 5:
                                print(f"       ... and {len(incompatible_keys.missing_keys) - 5} more")
                        
                        if incompatible_keys.unexpected_keys:
                            print(f"    âš ï¸  Warning: {len(incompatible_keys.unexpected_keys)} unexpected keys in projection_head checkpoint:")
                            for key in incompatible_keys.unexpected_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                print(f"       - {key}")
                            if len(incompatible_keys.unexpected_keys) > 5:
                                print(f"       ... and {len(incompatible_keys.unexpected_keys) - 5} more")
                        
                        # åªæœ‰å½“æ‰€æœ‰æƒé‡éƒ½å®Œç¾åŒ¹é…æ—¶æ‰æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                        if not incompatible_keys.missing_keys and not incompatible_keys.unexpected_keys:
                            print(f"    âœ“ All projection_head weights matched perfectly")
                        
                        # éªŒè¯å‚æ•°æ˜¯å¦è¢«æ­£ç¡®å†»ç»“
                        trainable = sum(p.numel() for p in model.projection_head.parameters() if p.requires_grad)
                        if trainable > 0:
                            print(f"    âš ï¸  Warning: projection_head has {trainable:,} trainable parameters (should be 0)")
                        else:
                            print(f"    âœ“ projection_head is correctly frozen")
                except Exception as e:
                    if self.is_main_process:
                        print(f"  âŒ Failed to load projection_head: {e}")
                        import traceback
                        traceback.print_exc()
            else:
                if self.is_main_process:
                    print(f"  âš ï¸  Model has no projection_head, skipping")
        else:
            if self.is_main_process:
                print(f"  âŒ projection_head.bin not found in {checkpoint_path}")
                print(f"     Stage 2 training requires projection_head from stage 1!")
        
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šåŠ è½½ special token embeddings
        # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒäº† <img_begin> å’Œ <img_end> embeddingsï¼Œå¿…é¡»åŠ è½½å®ƒä»¬
        special_tokens_file = checkpoint_path / "special_tokens.bin"
        if special_tokens_file.exists():
            try:
                special_tokens_state = torch.load(special_tokens_file, map_location='cpu')
                
                if self.is_main_process:
                    print(f"\n  Loading special token embeddings...")
                
                # è·å– embedding table
                embed_table = model.language_model.get_input_embeddings()
                
                # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„åŠ è½½
                loaded_embeddings = []
                missing_embeddings = []
                unexpected_keys = []
                
                # æ£€æŸ¥ checkpoint ä¸­çš„æ‰€æœ‰ keys
                expected_keys = {'img_begin_emb', 'img_end_emb', 'img_begin_token_id', 'img_end_token_id'}
                for key in special_tokens_state.keys():
                    if key not in expected_keys:
                        unexpected_keys.append(key)
                
                # åŠ è½½ <img_begin> embeddingï¼ˆåˆ° embedding table ä¸­ï¼‰
                if 'img_begin_emb' in special_tokens_state and hasattr(model, 'img_begin_token_id'):
                    embed_table.weight.data[model.img_begin_token_id].copy_(special_tokens_state['img_begin_emb'])
                    if self.is_main_process:
                        norm = embed_table.weight.data[model.img_begin_token_id].norm().item()
                        print(f"    âœ“ Loaded <img_begin> embedding (norm={norm:.4f})")
                        loaded_embeddings.append('img_begin_emb')
                elif 'img_begin_emb' not in special_tokens_state:
                    missing_embeddings.append('img_begin_emb')
                    if self.is_main_process:
                        print(f"    âš ï¸  Warning: 'img_begin_emb' not found in checkpoint")
                elif not hasattr(model, 'img_begin_token_id'):
                    if self.is_main_process:
                        print(f"    âš ï¸  Warning: Model has no 'img_begin_token_id' attribute")
                
                # åŠ è½½ <img_end> embeddingï¼ˆåˆ° embedding table ä¸­ï¼‰
                if 'img_end_emb' in special_tokens_state and hasattr(model, 'img_end_token_id'):
                    embed_table.weight.data[model.img_end_token_id].copy_(special_tokens_state['img_end_emb'])
                    if self.is_main_process:
                        norm = embed_table.weight.data[model.img_end_token_id].norm().item()
                        print(f"    âœ“ Loaded <img_end> embedding (norm={norm:.4f})")
                        loaded_embeddings.append('img_end_emb')
                elif 'img_end_emb' not in special_tokens_state:
                    missing_embeddings.append('img_end_emb')
                    if self.is_main_process:
                        print(f"    âš ï¸  Warning: 'img_end_emb' not found in checkpoint")
                elif not hasattr(model, 'img_end_token_id'):
                    if self.is_main_process:
                        print(f"    âš ï¸  Warning: Model has no 'img_end_token_id' attribute")
                
                # éªŒè¯ token IDs æ˜¯å¦åŒ¹é…
                if 'img_begin_token_id' in special_tokens_state and hasattr(model, 'img_begin_token_id'):
                    if special_tokens_state['img_begin_token_id'] != model.img_begin_token_id:
                        if self.is_main_process:
                            print(f"    âš ï¸  Warning: img_begin_token_id mismatch!")
                            print(f"       Checkpoint: {special_tokens_state['img_begin_token_id']}")
                            print(f"       Current model: {model.img_begin_token_id}")
                
                if 'img_end_token_id' in special_tokens_state and hasattr(model, 'img_end_token_id'):
                    if special_tokens_state['img_end_token_id'] != model.img_end_token_id:
                        if self.is_main_process:
                            print(f"    âš ï¸  Warning: img_end_token_id mismatch!")
                            print(f"       Checkpoint: {special_tokens_state['img_end_token_id']}")
                            print(f"       Current model: {model.img_end_token_id}")
                
                # æ±‡æ€»æŠ¥å‘Š
                if self.is_main_process:
                    if missing_embeddings:
                        print(f"    âš ï¸  Warning: {len(missing_embeddings)} embeddings missing in checkpoint: {missing_embeddings}")
                    
                    if unexpected_keys:
                        print(f"    âš ï¸  Warning: {len(unexpected_keys)} unexpected keys in checkpoint: {unexpected_keys}")
                    
                    # åªæœ‰å½“æ‰€æœ‰é¢„æœŸçš„ embeddings éƒ½åŠ è½½æˆåŠŸä¸”æ²¡æœ‰ unexpected keys æ—¶æ‰æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                    if len(loaded_embeddings) == 2 and not missing_embeddings and not unexpected_keys:
                        print(f"  âœ“ All special token embeddings loaded successfully")
                    elif loaded_embeddings:
                        print(f"  âš ï¸  Partial success: loaded {len(loaded_embeddings)}/2 embeddings")
                    
            except Exception as e:
                if self.is_main_process:
                    print(f"  âŒ Failed to load special token embeddings: {e}")
                    print(f"     This will cause high initial loss in Stage 2!")
                    import traceback
                    traceback.print_exc()
        else:
            if self.is_main_process:
                print(f"\n  âš ï¸  special_tokens.bin not found in {checkpoint_path}")
                print(f"     This checkpoint may be from an old version.")
                print(f"     Special token embeddings will use random initialization!")
                print(f"     Expected high initial loss - consider re-training Stage 1.")
        
        # 2. åŠ è½½ LoRA weightsï¼ˆå¦‚æœå­˜åœ¨ä¸”ç¬¬ä¸€é˜¶æ®µä½¿ç”¨äº† LoRAï¼‰
        adapter_config_file = checkpoint_path / "adapter_config.json"
        adapter_model_file = checkpoint_path / "adapter_model.bin"
        
        if adapter_config_file.exists() and adapter_model_file.exists():
            if self.is_main_process:
                print(f"\n  Found LoRA weights from stage 1")
                print(f"  Note: Stage 2 does not use LoRA, but loading for reference...")
            
            try:
                # è¯»å– adapter_config æŸ¥çœ‹ä¿¡æ¯
                import json
                with open(adapter_config_file, 'r') as f:
                    adapter_config = json.load(f)
                    if self.is_main_process:
                        print(f"    LoRA rank: {adapter_config.get('r', 'unknown')}")
                        print(f"    LoRA alpha: {adapter_config.get('lora_alpha', 'unknown')}")
                        print(f"    Target modules: {adapter_config.get('target_modules', 'unknown')}")
                
                # æ³¨æ„ï¼šç¬¬äºŒé˜¶æ®µä¸ä½¿ç”¨ LoRAï¼Œæ‰€ä»¥ä¸åŠ è½½è¿™äº›æƒé‡
                if self.is_main_process:
                    print(f"    â„¹ï¸  LoRA weights not loaded (stage 2 trains lm_head only)")
            except Exception as e:
                if self.is_main_process:
                    print(f"    âš ï¸  Could not read LoRA config: {e}")
        
        # 3. åŠ è½½è®­ç»ƒçŠ¶æ€ä¿¡æ¯ï¼ˆç”¨äºè®°å½•ï¼‰
        training_state_file = checkpoint_path / "training_state.json"
        if training_state_file.exists():
            try:
                with open(training_state_file, 'r') as f:
                    training_state = json.load(f)
                    if self.is_main_process:
                        print(f"\n  Stage 1 Training Info:")
                        print(f"    Global step: {training_state.get('global_step', 'unknown')}")
                        print(f"    Best loss: {training_state.get('best_loss', 'unknown')}")
                        if 'use_lora' in training_state:
                            print(f"    Used LoRA: {training_state.get('use_lora', False)}")
            except Exception as e:
                if self.is_main_process:
                    print(f"    âš ï¸  Could not read training state: {e}")
        
        # 4. éªŒè¯åŠ è½½åçš„æ¨¡å‹çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…éå†æ‰€æœ‰å‚æ•°å¯¼è‡´å†…å­˜é—®é¢˜ï¼‰
        # æ³¨æ„ï¼šåœ¨ DeepSpeed ZeRO æ¨¡å¼ä¸‹ï¼Œéå†æ‰€æœ‰å‚æ•°å¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜
        final_lm_head_trainable = 0
        
        # æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ£€æŸ¥ï¼Œä½†åªæœ‰ä¸»è¿›ç¨‹æ‰“å°
        try:
            if hasattr(model, 'vision_encoder') and hasattr(model.vision_encoder, 'mllm_model'):
                if hasattr(model.vision_encoder.mllm_model, 'lm_head'):
                    lm_head = model.vision_encoder.mllm_model.lm_head
                    # åªæ£€æŸ¥å‚æ•°æ•°é‡ï¼Œä½¿ç”¨ list() é¿å…ç”Ÿæˆå™¨é—®é¢˜
                    try:
                        lm_head_params = list(lm_head.parameters())
                        if lm_head_params:  # ç¡®ä¿æœ‰å‚æ•°
                            lm_head_trainable = sum(p.numel() for p in lm_head_params if p.requires_grad)
                            final_lm_head_trainable = lm_head_trainable
                            
                            if self.is_main_process:
                                lm_head_total = sum(p.numel() for p in lm_head_params)
                                print(f"\n  {'='*56}")
                                print("  Verifying Model State for Stage 2 Training")
                                print(f"  {'='*56}")
                                print(f"\n  Direct lm_head check:")
                                print(f"    lm_head.trainable: {lm_head_trainable:,} / {lm_head_total:,}")
                                if lm_head_trainable > 0:
                                    print(f"    âœ“ lm_head is trainable")
                                    print(f"\n  Stage 2 Requirements Check:")
                                    print(f"    âœ“ lm_head is trainable ({lm_head_trainable:,} parameters)")
                                else:
                                    print(f"    âŒ lm_head is NOT trainable")
                                    print(f"\n  Stage 2 Requirements Check:")
                                    print(f"    âŒ lm_head is NOT trainable (ERROR!)")
                        else:
                            if self.is_main_process:
                                print(f"\n  {'='*56}")
                                print("  Verifying Model State for Stage 2 Training")
                                print(f"  {'='*56}")
                                print(f"    âš ï¸  lm_head has no parameters")
                    except Exception as e:
                        if self.is_main_process:
                            print(f"    âš ï¸  Error checking lm_head parameters: {e}")
                else:
                    if self.is_main_process:
                        print(f"\n  {'='*56}")
                        print("  Verifying Model State for Stage 2 Training")
                        print(f"  {'='*56}")
                        print(f"    âš ï¸  Cannot find mllm_model.lm_head")
            else:
                if self.is_main_process:
                    print(f"\n  {'='*56}")
                    print("  Verifying Model State for Stage 2 Training")
                    print(f"  {'='*56}")
                    print(f"    âš ï¸  Cannot find vision_encoder.mllm_model")
        except Exception as e:
            if self.is_main_process:
                print(f"    âš ï¸  Error accessing lm_head: {e}")
        
        # 5. å¦‚æœ lm_head ä¸å¯è®­ç»ƒï¼Œå°è¯•é‡æ–°å¯ç”¨ï¼ˆå¯èƒ½åœ¨ DeepSpeed åˆå§‹åŒ–åè¢«é‡ç½®ï¼‰
        if self.config["training"].get("train_lm_head_only", False) and final_lm_head_trainable == 0:
            if self.is_main_process:
                print(f"\n  âš ï¸  lm_head is not trainable, attempting to re-enable...")
            
            try:
                # ç›´æ¥è®¿é—®å¹¶é‡æ–°å¯ç”¨ lm_head çš„æ¢¯åº¦
                if hasattr(model, 'vision_encoder') and hasattr(model.vision_encoder, 'mllm_model'):
                    if hasattr(model.vision_encoder.mllm_model, 'lm_head'):
                        lm_head = model.vision_encoder.mllm_model.lm_head
                        # ä½¿ç”¨ list() é¿å…ç”Ÿæˆå™¨é—®é¢˜
                        lm_head_params = list(lm_head.parameters())
                        for param in lm_head_params:
                            param.requires_grad = True
                        
                        # éªŒè¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…é‡å¤éå†ï¼‰
                        if lm_head_params:
                            recheck_trainable = sum(p.numel() for p in lm_head_params if p.requires_grad)
                            if self.is_main_process:
                                if recheck_trainable > 0:
                                    print(f"    âœ“ Successfully re-enabled lm_head ({recheck_trainable:,} parameters)")
                                else:
                                    print(f"    âŒ Failed to re-enable lm_head")
            except Exception as e:
                if self.is_main_process:
                    print(f"    âš ï¸  Error re-enabling lm_head: {e}")
        
        if self.is_main_process:
            print(f"{'='*60}\n")
    
    def load_checkpoint(self, checkpoint_path: str):
        """
        åŠ è½½æ£€æŸ¥ç‚¹å¹¶æ¢å¤è®­ç»ƒçŠ¶æ€
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. Stage 1 æ–­ç‚¹ç»­è®­ï¼šåŠ è½½ projection_head + special_tokens + è®­ç»ƒçŠ¶æ€
        2. Stage 2 æ–­ç‚¹ç»­è®­ï¼šåŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆDeepSpeed checkpoint æˆ– LoRA weightsï¼‰
        """
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            raise ValueError(f"Checkpoint path does not exist: {checkpoint_path}")
        
        if self.is_main_process:
            print("\n" + "="*80)
            print("Resume Training from Checkpoint")
            print("="*80)
            print(f"Checkpoint path: {checkpoint_path}")
        
        # æ£€æŸ¥è®­ç»ƒé˜¶æ®µï¼ˆstage1 or stage2ï¼‰
        model_to_check = self.model.module if hasattr(self.model, "module") else self.model
        if self.model_engine is not None:
            model_to_check = self.model_engine.module
        is_stage1 = not getattr(model_to_check, 'stage2_mode', False)
        
        if self.is_main_process:
            print(f"Training stage: {'Stage 1' if is_stage1 else 'Stage 2'}")
        
        # Stage 1 æ–­ç‚¹ç»­è®­
        if is_stage1:
            self._resume_stage1_training(checkpoint_path)
        # Stage 2 æ–­ç‚¹ç»­è®­
        else:
            self._resume_stage2_training(checkpoint_path)
        
        if self.is_main_process:
            print("="*80)
            print(f"âœ“ Successfully resumed training from step {self.global_step}")
            print(f"  Best loss so far: {self.best_loss:.4f}")
            print("="*80 + "\n")
    
    def _resume_stage1_training(self, checkpoint_path: Path):
        """æ¢å¤ Stage 1 è®­ç»ƒ"""
        if self.is_main_process:
            print("\nğŸ“‚ Loading Stage 1 checkpoint components...")
        
        # è·å–å®é™…æ¨¡å‹ï¼ˆå¤„ç† DeepSpeed åŒ…è£…ï¼‰
        if self.use_deepspeed and self.model_engine is not None:
            model = self.model_engine.module
        else:
            model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # 1. åŠ è½½è®­ç»ƒçŠ¶æ€ï¼ˆglobal_step, best_lossï¼‰
        state_file = checkpoint_path / "training_state.json"
        if state_file.exists():
            with open(state_file, "r") as f:
                training_state = json.load(f)
                self.global_step = training_state.get("global_step", 0)
                self.best_loss = training_state.get("best_loss", float("inf"))
                if self.is_main_process:
                    print(f"  âœ“ Training state loaded")
                    print(f"    - Global step: {self.global_step}")
                    print(f"    - Best loss: {self.best_loss:.4f}")
        else:
            if self.is_main_process:
                print(f"  âš ï¸  training_state.json not found")
            
            # å°è¯•ä» DeepSpeed checkpoint ç›®å½•åæ¨æ–­ global_step
            # ç›®å½•ç»“æ„å¯èƒ½æ˜¯ checkpoint_step_2000/global_step2000/ æˆ–ç±»ä¼¼çš„
            # æˆ–è€…æ›´å¸¸è§çš„æ˜¯ checkpoint_path æœ¬èº«åŒ…å« step ä¿¡æ¯ï¼Œå¦‚ checkpoint_step_2000
            try:
                # 1. å…ˆå°è¯•ä» checkpoint_path åå­—æ¨æ–­ (å¦‚ checkpoint_step_2000)
                step_match = re.search(r"checkpoint_step_(\d+)", checkpoint_path.name)
                if step_match:
                    self.global_step = int(step_match.group(1))
                    if self.is_main_process:
                        print(f"  âœ“ Inferred global_step={self.global_step} from directory name '{checkpoint_path.name}'")
                else:
                    # 2. å°è¯•ä»å†…éƒ¨çš„ global_step* ç›®å½•æ¨æ–­
                    ds_step_dirs = list(checkpoint_path.glob("global_step*"))
                    if ds_step_dirs:
                        # å–æœ€å¤§çš„æ•°å­—
                        latest_dir = max(ds_step_dirs, key=lambda x: int(re.search(r"(\d+)", x.name).group(1) if re.search(r"(\d+)", x.name) else 0))
                        step_match = re.search(r"(\d+)", latest_dir.name)
                        if step_match:
                            # æ³¨æ„ï¼šDeepSpeed çš„ step å¯èƒ½æ˜¯ micro-step è¿˜æ˜¯ global-step å–å†³äºé…ç½®
                            # ä½†é€šå¸¸å¯ä»¥ç›´æ¥ç”¨ä½œ global_step
                            self.global_step = int(step_match.group(1))
                            if self.is_main_process:
                                print(f"  âœ“ Inferred global_step={self.global_step} from DeepSpeed directory '{latest_dir.name}'")
            except Exception as e:
                if self.is_main_process:
                    print(f"  âš ï¸  Failed to infer global_step: {e}")
                    print(f"     Starting from step 0")
        
        # 2. åŠ è½½ projection_head
        projection_file = checkpoint_path / "projection_head.bin"
        if projection_file.exists() and model.projection_head is not None:
            try:
                state_dict = torch.load(projection_file, map_location='cpu')
                model.projection_head.load_state_dict(state_dict)
                
                # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if not self.use_deepspeed:
                    model.projection_head = model.projection_head.to(self.device)
                
                if self.is_main_process:
                    param_count = sum(p.numel() for p in model.projection_head.parameters())
                    print(f"  âœ“ Projection head loaded ({param_count:,} parameters)")
            except Exception as e:
                if self.is_main_process:
                    print(f"  âŒ Failed to load projection_head: {e}")
                raise
        else:
            if self.is_main_process:
                print(f"  âš ï¸  projection_head.bin not found")
        
        # 3. åŠ è½½ special token embeddings
        special_tokens_file = checkpoint_path / "special_tokens.bin"
        if special_tokens_file.exists():
            try:
                special_tokens_state = torch.load(special_tokens_file, map_location='cpu')
                embed_table = model.language_model.get_input_embeddings()
                
                if 'img_begin_emb' in special_tokens_state and hasattr(model, 'img_begin_token_id'):
                    embed_table.weight.data[model.img_begin_token_id].copy_(special_tokens_state['img_begin_emb'])
                    if self.is_main_process:
                        norm = embed_table.weight.data[model.img_begin_token_id].norm().item()
                        print(f"  âœ“ <img_begin> embedding loaded (norm={norm:.4f})")
                
                if 'img_end_emb' in special_tokens_state and hasattr(model, 'img_end_token_id'):
                    embed_table.weight.data[model.img_end_token_id].copy_(special_tokens_state['img_end_emb'])
                    if self.is_main_process:
                        norm = embed_table.weight.data[model.img_end_token_id].norm().item()
                        print(f"  âœ“ <img_end> embedding loaded (norm={norm:.4f})")
            except Exception as e:
                if self.is_main_process:
                    print(f"  âš ï¸  Failed to load special token embeddings: {e}")
        else:
            if self.is_main_process:
                print(f"  âš ï¸  special_tokens.bin not found")
        
        # 4. DeepSpeed æ¨¡å¼ï¼šæ¢å¤ optimizer å’Œ scheduler çŠ¶æ€
        if self.use_deepspeed and self.model_engine is not None:
            # æ£€æŸ¥æ˜¯å¦æœ‰ DeepSpeed checkpoint
            global_step_dirs = list(checkpoint_path.glob("global_step_*"))
            if global_step_dirs or (checkpoint_path / "latest").exists():
                if self.is_main_process:
                    print(f"\n  ğŸ“¦ Loading DeepSpeed optimizer/scheduler state...")
                try:
                    # æŸ¥æ‰¾æ­£ç¡®çš„ tag
                    if global_step_dirs:
                        latest_step_dir = max(global_step_dirs, key=lambda x: int(x.name.split("_")[-1]))
                        tag = latest_step_dir.name
                        load_path = str(checkpoint_path)
                    elif (checkpoint_path / "latest").exists():
                        with open(checkpoint_path / "latest", "r") as f:
                            tag = f.read().strip()
                        load_path = str(checkpoint_path)
                    else:
                        load_path = str(checkpoint_path)
                        tag = None
                    
                    # åŠ è½½ DeepSpeed checkpointï¼ˆä¼šæ¢å¤ optimizer å’Œ schedulerï¼‰
                    _, client_state = self.model_engine.load_checkpoint(load_path, tag=tag)
                    
                    # æ›´æ–°è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æœ client_state ä¸­æœ‰ï¼‰
                    if client_state:
                        self.global_step = client_state.get("global_step", self.global_step)
                        self.best_loss = client_state.get("best_loss", self.best_loss)
                    
                    if self.is_main_process:
                        print(f"  âœ“ DeepSpeed state loaded (optimizer + scheduler)")
                except Exception as e:
                    if self.is_main_process:
                        print(f"  âš ï¸  Failed to load DeepSpeed state: {e}")
                        print(f"     Will start with fresh optimizer/scheduler")
            else:
                if self.is_main_process:
                    print(f"  â„¹ï¸  No DeepSpeed checkpoint found, using fresh optimizer/scheduler")
        
        # 5. é DeepSpeed æ¨¡å¼ï¼šæ¢å¤ optimizer å’Œ scheduler
        else:
            optimizer_file = checkpoint_path / "optimizer.bin"
            if optimizer_file.exists() and self.optimizer is not None and self.scheduler is not None:
                try:
                    optimizer_state = torch.load(optimizer_file, map_location=self.device)
                    self.optimizer.load_state_dict(optimizer_state["optimizer_state_dict"])
                    self.scheduler.load_state_dict(optimizer_state["scheduler_state_dict"])
                    if self.is_main_process:
                        print(f"  âœ“ Optimizer and scheduler state loaded")
                except Exception as e:
                    if self.is_main_process:
                        print(f"  âš ï¸  Failed to load optimizer/scheduler: {e}")
            else:
                if self.is_main_process:
                    print(f"  â„¹ï¸  No optimizer.bin found, using fresh optimizer/scheduler")
    
    def _resume_stage2_training(self, checkpoint_path: Path):
        """æ¢å¤ Stage 2 è®­ç»ƒ
        
        å‚è€ƒ evaluate.py çš„åŠ è½½é€»è¾‘ï¼Œç¡®ä¿ä¸æ¨ç†æ—¶çš„åŠ è½½æ–¹å¼ä¸€è‡´
        """
        if self.is_main_process:
            print(f"\nğŸ“‚ Resuming Stage 2 training from {checkpoint_path}")
        
        # 1. å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€ (global_step, best_loss)
        # ä¼˜å…ˆä» training_state.json è¯»å–
        state_file = checkpoint_path / "training_state.json"
        if state_file.exists():
            try:
                with open(state_file, "r") as f:
                    training_state = json.load(f)
                    self.global_step = training_state.get("global_step", 0)
                    self.best_loss = training_state.get("best_loss", float("inf"))
                    if self.is_main_process:
                        print(f"  âœ“ Loaded training state: step={self.global_step}, best_loss={self.best_loss:.4f}")
            except Exception as e:
                if self.is_main_process:
                    print(f"  âš ï¸  Failed to load training_state.json: {e}")
        else:
            # å°è¯•ä»ç›®å½•åæ¨æ–­ step
            try:
                global_step_dirs = sorted(checkpoint_path.glob("global_step*"))
                if global_step_dirs:
                    latest = global_step_dirs[-1]
                    step_str = latest.name.replace("global_step", "")
                    self.global_step = int(step_str)
                    if self.is_main_process:
                        print(f"  âš ï¸  Inferred global_step={self.global_step} from directory name")
            except:
                pass

        # 2. [DeepSpeed] åŠ è½½ Optimizer å’Œ Scheduler çŠ¶æ€
        # è¿™ä¸€æ­¥ä¹Ÿä¼šå°è¯•åŠ è½½æ¨¡å‹æƒé‡ï¼Œä½†ç”±äº Key Mismatch é—®é¢˜ï¼Œæƒé‡å¯èƒ½ä¸æ­£ç¡®
        # æˆ‘ä»¬ç¨åä¼šæ‰‹åŠ¨è¦†ç›–æƒé‡
        if self.use_deepspeed and self.model_engine is not None:
            if self.is_main_process:
                print(f"  ğŸ”„ Calling model_engine.load_checkpoint()...")
            
            try:
                load_path = str(checkpoint_path)
                # load_checkpoint è¿”å› (load_path, client_state)
                _, client_state = self.model_engine.load_checkpoint(load_path)
                
                if self.is_main_process:
                    print(f"  âœ“ DeepSpeed load_checkpoint completed (Optimizer/Scheduler loaded)")
                
                # å¦‚æœä¹‹å‰æ²¡è¯»åˆ° training_stateï¼Œå°è¯•ä» client_state è¯»å–
                if client_state and (not state_file.exists() or self.global_step == 0):
                    self.global_step = client_state.get("global_step", self.global_step)
                    self.best_loss = client_state.get("best_loss", self.best_loss)
                    if self.is_main_process:
                        print(f"  âœ“ Loaded state from DeepSpeed client_state: step={self.global_step}")

            except Exception as e:
                if self.is_main_process:
                    print(f"  âŒ DeepSpeed load_checkpoint failed: {e}")
                    print(f"     This is critical for resuming optimizer state!")
                # è¿™é‡Œæˆ‘ä»¬ä¸ raiseï¼Œå°è¯•ç»§ç»­åŠ è½½æƒé‡ï¼Œä¹Ÿè®¸èƒ½è·‘ï¼ˆè™½ç„¶ optimizer ä¸¢å¤±ï¼‰
                # ä½†é€šå¸¸åº”è¯¥ raise

        # 3. [å…³é”®ä¿®å¤] æ‰‹åŠ¨åŠ è½½ Stage 2 æ¨¡å‹æƒé‡
        # ä½¿ç”¨ evaluate.py ä¸­çš„é€»è¾‘æ¥å¤„ç† Key Mismatch
        if self.is_main_process:
            print(f"  ğŸ”§ Manually loading Stage 2 weights (fixing key mismatches)...")

        try:
            # å¯»æ‰¾ model states æ–‡ä»¶
            ds_dirs = sorted(checkpoint_path.glob("global_step*"))
            latest_step_dir = None
            
            if ds_dirs:
                latest_step_dir = ds_dirs[-1]
            elif (checkpoint_path / "latest").exists():
                with open(checkpoint_path / "latest", "r") as f:
                    tag = f.read().strip()
                latest_step_dir = checkpoint_path / tag
            
            if latest_step_dir and (latest_step_dir / "mp_rank_00_model_states.pt").exists():
                model_states_file = latest_step_dir / "mp_rank_00_model_states.pt"
                if self.is_main_process:
                    print(f"  Loading weights from: {model_states_file}")
                
                # åŠ è½½ checkpoint
                checkpoint_state = torch.load(model_states_file, map_location="cpu")
                
                # æå– state_dict
                state_dict = None
                for key in ["module", "model_state_dict", "model", "state_dict"]:
                    if key in checkpoint_state:
                        state_dict = checkpoint_state[key]
                        break
                if state_dict is None:
                    state_dict = checkpoint_state
                
                # æ¸…ç†å’Œæ ‡å‡†åŒ– Keys (å‚è€ƒ evaluate.py)
                normalized_state = {}
                keys_normalized = 0
                
                for key, value in state_dict.items():
                    clean_key = key
                    # ç§»é™¤å‰ç¼€
                    for prefix in ["module.", "model.", "vision_encoder.mllm_model."]:
                        if clean_key.startswith(prefix):
                            clean_key = clean_key[len(prefix):]
                            break
                    
                    # è·³è¿‡é mllm_model ç»„ä»¶
                    if any(c in key for c in ["vision_encoder.ocr", "projection_head", "vision_encoder.clip", "text_renderer"]):
                        continue

                    # æ ‡å‡†åŒ– logic
                    normalized_key = clean_key
                    # æƒ…å†µ1: å®Œå…¨æ²¡æœ‰ base_model.model. å‰ç¼€
                    if not clean_key.startswith("base_model.model."):
                        if any(clean_key.startswith(p) for p in ["language_model.", "visual.", "llm_lm_head"]):
                            normalized_key = f"base_model.model.model.{clean_key}"
                            keys_normalized += 1
                    # æƒ…å†µ2: æœ‰ base_model.model. ä½†ç¼ºå°‘ä¸­é—´çš„ model.
                    elif clean_key.startswith("base_model.model.") and not clean_key.startswith("base_model.model.model."):
                        suffix = clean_key[len("base_model.model."):]
                        if any(suffix.startswith(p) for p in ["language_model.", "visual.", "llm_lm_head"]):
                            normalized_key = f"base_model.model.model.{suffix}"
                            keys_normalized += 1
                            
                    normalized_state[normalized_key] = value

                if self.is_main_process:
                    print(f"  Normalized {keys_normalized} keys. Total keys: {len(normalized_state)}")
                
                # è·å–å®é™…æ¨¡å‹
                model_to_load = self.model.module if hasattr(self.model, "module") else self.model
                # ç¡®ä¿ vision_encoder å·²åŠ è½½
                if not hasattr(model_to_load, 'vision_encoder') or not hasattr(model_to_load.vision_encoder, 'mllm_model'):
                     print("  âš ï¸  Model structure unexpected: missing vision_encoder.mllm_model")
                else:
                    mllm_model = model_to_load.vision_encoder.mllm_model
                    
                    # åŠ è½½åˆ° mllm_model (Peft æˆ– Full)
                    missing, unexpected = mllm_model.load_state_dict(normalized_state, strict=False)
                    
                    if self.is_main_process:
                        # è¿‡æ»¤æ‰ expected missing keys
                        important_missing = [k for k in missing if not k.startswith("base_model.model.")]
                        if important_missing:
                             print(f"    âš ï¸  Missing keys (subset): {important_missing[:5]}...")
                        print(f"  âœ… Manual weight loading completed")

            else:
                if self.is_main_process:
                    print(f"  âš ï¸  Could not find mp_rank_00_model_states.pt in {latest_step_dir}")

        except Exception as e:
            if self.is_main_process:
                print(f"  âŒ Manual weight loading failed: {e}")
                import traceback
                traceback.print_exc()
                
        # åŒæ­¥
        if self.use_deepspeed:
            torch.distributed.barrier()


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    return config


def main():
    parser = argparse.ArgumentParser(description="Train CoT Compressor")
    parser.add_argument("--config", type=str, default="configs/default_config.yaml", help="Config file path")
    parser.add_argument("--dataset", type=str, default=None, help="Dataset name (override config)")
    parser.add_argument("--batch_size", type=int, default=None, help="Batch size (override config)")
    parser.add_argument("--num_epochs", type=int, default=None, help="Number of epochs (override config)")
    parser.add_argument("--lr", type=float, default=None, help="Learning rate (override config)")
    parser.add_argument("--deepspeed_config", type=str, default=None, help="DeepSpeed config file path")
    parser.add_argument(
        "--save_interval", type=int, default=None, help="Save checkpoint every N steps (override config)"
    )
    parser.add_argument(
        "--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint directory to resume training from"
    )

    # DeepSpeed å’Œåˆ†å¸ƒå¼è®­ç»ƒå‚æ•°ï¼ˆDeepSpeed ä¼šè‡ªåŠ¨ä¼ é€’è¿™äº›å‚æ•°ï¼‰
    # ä½¿ç”¨ parse_known_args æ¥å¿½ç•¥æœªçŸ¥å‚æ•°ï¼Œé¿å… DeepSpeed ä¼ é€’çš„å‚æ•°å¯¼è‡´é”™è¯¯
    parser.add_argument(
        "--local_rank", type=int, default=-1, help="Local rank for distributed training (set by DeepSpeed)"
    )

    # ä½¿ç”¨ parse_known_args æ¥å¤„ç† DeepSpeed å¯èƒ½ä¼ é€’çš„é¢å¤–å‚æ•°
    args, unknown_args = parser.parse_known_args()

    # å¦‚æœæœ‰æœªçŸ¥å‚æ•°ï¼Œæ‰“å°è­¦å‘Šï¼ˆä½†ä¸æŠ¥é”™ï¼‰
    if unknown_args:
        if any("local_rank" in arg or "local-rank" in arg for arg in unknown_args):
            # DeepSpeed ä¼ é€’çš„ local_rank å‚æ•°ï¼Œå¿½ç•¥
            pass
        else:
            print(f"Warning: Unknown arguments ignored: {unknown_args}")

    # å¦‚æœæä¾›äº† DeepSpeed é…ç½®ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
    if args.deepspeed_config is None:
        args.deepspeed_config = None  # å°†åœ¨ Trainer ä¸­ä» config è¯»å–

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.dataset:
        config["data"]["dataset_name"] = args.dataset
    if args.batch_size:
        config["training"]["batch_size"] = args.batch_size
    if args.num_epochs:
        config["training"]["num_epochs"] = args.num_epochs
    if args.lr:
        config["training"]["learning_rate"] = args.lr
    if args.save_interval:
        config["logging"]["save_interval"] = args.save_interval

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config["misc"]["seed"])
    np.random.seed(config["misc"]["seed"])

    # ç¡®å®š DeepSpeed é…ç½®æ–‡ä»¶è·¯å¾„
    deepspeed_config_path = args.deepspeed_config
    if deepspeed_config_path is None:
        deepspeed_config_path = config.get("deepspeed", {}).get("config_file", None)

    # æ‰“å°é…ç½®
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if local_rank <= 0:  # åªåœ¨ä¸»è¿›ç¨‹æˆ–å•å¡æ¨¡å¼ä¸‹æ‰“å°
        print("=" * 80)
        print("Configuration")
        print("=" * 80)
        print(f"Dataset: {config['data']['dataset_name']}")
        print(f"Batch size: {config['training']['batch_size']}")
        print(f"Num epochs: {config['training']['num_epochs']}")
        print(f"Learning rate: {config['training']['learning_rate']}")
        print(f"Save interval: {config['logging']['save_interval']} steps")
        print(f"Local rank: {local_rank}")
        print(f"DeepSpeed config: {deepspeed_config_path}")
        print(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
        print("=" * 80)

    # è®­ç»ƒ
    trainer = Trainer(config, deepspeed_config_path=deepspeed_config_path)
    
    # å¦‚æœæä¾›äº†æ¢å¤æ£€æŸ¥ç‚¹è·¯å¾„ï¼ŒåŠ è½½æ£€æŸ¥ç‚¹
    if args.resume_from_checkpoint:
        trainer.load_checkpoint(args.resume_from_checkpoint)
    
    trainer.train()


if __name__ == "__main__":
    main()
